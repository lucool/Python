Day1-IPython与Numpy

## Anaconda

下载地址： https://www.anaconda.com/distribution/#download-section

Anaconda 2018.12 版本说明	
	包含了1500+ Python/R的用于科学技术的数据包
	bconda工具，便于管理依赖库及创建虚拟环境
	开发与训练机器学习和深度学习
		scikit-learn
		tensorflow
		theano
	数据分析的开发环境库
		Dask
		Numpy
		Numba
		pandas
	可视化结果
		Matplotlib
		Bokeh https://www.cnblogs.com/alexzhang92/p/9794373.html
		DataShader 安装 https://pypi.org/project/datashader/
		Holoviews 官网 http://holoviews.org/
	支持的操作系统
		Window
		Linux
		Mac
## 环境配置

环境变量名是全大写字母 PATH

- 将 {Anaconda_home}/bin目录添加到环境变量中的path中

- Linux
  - 用户环境变量文件:  ~/.bashrc
  - 全局环境变量文件: /etc/.bashrc
- mac
  - 用户变量文件:  ~/.bash_profile 

## conda工具应用

### 软件源-清华源

https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes

### 创建环境

- conda create --name 虚拟环境名  python=3.6.3
  - 指定python的版本
  - 此环境是一个干净的Python基础环境;      pip list 查看已安装的包
- conda create --name 虚拟环境名  python=3.6.3  anaconda
  - 创建环境时，会安装anaconda相关的包

### 激活/切换环境

conda activate 虚拟环境名

### 退出环境

conda deactivate
注意： 默认会退到base环境下，继续执行上面命令，可以退出系统环境

### 克隆环境

​		conda create -n 新环境名 --clone 原环境名

### 删除环境

​		conda remove -n 环境名  --all

### 小技巧

- 新的开发环境会被默认安装在你conda目录下的envs文件目录下
- conda create -h  查看创建环境的帮助
- conda info -envis或者(-e)  列出所有的环境
- conda create -n 环境名 python=3
  - 创建python3下的虚拟环境
- pip加速
  - linux
    - mkdir ~/.pip/vi ~/.pip/pip.conf
      - [global]index-url=https://mirrors.aliyun.com/pypi/simple
      - [install]trusted-host=mirrors.aliyun.com
  - window
    - 在当前用户的home目录(c:\Users\xxxx)下创建pip目录
    - 创建并编辑 pip.ini文件，内容同
- bz2文件库安装
  - 可以在 https://anaconda.org/搜索相关库
  - conda install --use-local xxx.tar.bz2

## IPython与Numpy

> 为什么使用python进行数据分析？

- python大量的库为数据分析和处理提供了完整的工具集
- 比起R和Matlab等其他主要用于数据分析的编程语言，Python更全能
  - Python不仅提供数据处理平台，而且还有其他语言和专业应用所没有的应用。
    - 可以用作脚本
    - 可以操作数据库
    - 可以开发web应用
- Python库一直在增加，算法的实现采用更具创新性的方法
- Python能和很多语言对接，例如高效的C语言
  	

### 什么是Ipython

- ipython是一个性能强大的python终端
  - ipython shell：功能强大的交互式shell
         $ ipython
  - ipython notebook：集文本、代码、图像、公式的展现于一体的超级python web界面
    	从ipython4.0开始改名成Jupyter notebook

### 什么是Jupyter

> Jupyter notebook：集文本、代码、图像、公式的展现于一体的超级python web界面

### Ipython

- 启动：ipython/jupyter  notebook
  - -h 查看帮助
  - password 配置口令
  - --ip 配置服务绑定的ip
    - 默认为localhost/127.0.0.1
  - --port 绑定服务的port
    - 默认8888
  - Ipython帮助文档
    - Help(参数)方法    [eg:help(len)]
    - ?
    - ??可以显示源码
    - tab自动补全

### Ipython魔法指令

- 运行外部Python文件：%run a.py（当前路径）

  - 运行其他路径：%run /home/nanfengpo/Desktop/bb.py
    尤其要注意的是，当我们使用魔法命令执行了一个外部文件时该文件的函数就能在当前会话中使

- 运行计时

  - 用下面命令计算statement的运行时间：
    %time statement

  - 用下面命令计算statement的平均运行时间：
    %timeit statement

    - %time一般用于耗时长的代码段
    - %timeit一般用于耗时短的代码段

  - 可以使用两个百分号来测试多行代码的平均运行时间：
    %%timeit
    statement1
    statement2
    statement3

  - 查看当前会话中的所有变量与函数

    - 快速查看当前会话的所有变量与函数名称：
      %who
    - 查看当前会话的所有变量与函数名称的详细信息：
      %whos
    - 返回一个字符串列表，里面元素是当前会话的所有变量与函数名称：
      %who_ls

  - 执行Linux指令使用！
    	!pwd
      	!ls	

    ```
    Linux指令：
    
    $ echo "hello world"        
    hello world
    
    $ pwd                           
    /home/jake 
    
    $ ls
    notebooks  projects 
    
    
    $ mkdir mm						#创建新路径
    ##桌面上创建文件夹
    !mkdir ../../../../Desktop/earth
    
    $ gedit 1.py						#创建文件并打开
    
    在Linux指令之前加上感叹号，即可在ipython当中执行Linux指令。
    注意会将标准输出以字符串形式返回
    ```

  - 更多魔法指令
    lsmagic

    ```
    列出所有魔法命令
    lsmagic
    ---------------------------
    
    Available line magics:
    %alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode
    
    Available cell magics:
    %%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile
    
    Automagic is ON, % prefix IS NOT needed for line magics.
    ```

    

### Jupyter notebook快捷键

• Enter : 转入编辑模式 
• Shift-Enter : 运行本单元，选中下个单元
• Y : 单元转入代码状态
• M :单元转入markdown状态
• O :  关闭或打开执行的结果
• A : 在上方插入新单元
• B : 在下方插入新单元
• Double-D：删除一行
• Ctrl-A : 全选
• Ctrl-Z : 复原
• Shift-Enter : 运行本单元，选中下一单元
• Ctrl-Enter : 运行本单元
• Alt-Enter : 运行本单元，在下面插入一单元
• Shift+Tab: 查看函数的帮助

### Numpy

> 什么是Numpy：Numeric Python			

- NumPy系统是Python的一种开源的数值计算扩展
- 优势
  - 一个强大的N维数组对象Array
  - 比较成熟的（广播）函数库
  - 用于整合C/C++和Fortran代码的工具包
  - 实用的最优化、积分、插值、线性代数、傅里叶变换和随机数生成函数
  - numpy和稀疏矩阵运算包scipy配合使用更加强大

> 导入

- import numpy as np
- 查看版本 np.__version__

> 创建ndarray

- 使用np.array()

  ```
  #一维
  import numpy as np
  test = np.array([1,2,3,4,5])
  test
  ////////////////////////////////
  #多维
  test = np.array([[1,2,3],[4,5,6]])
  test
  ```

  注意:

  numpy默认ndarray的所有元素的类型是相同的
  如果传进来的列表中包含不同的类型，则统一为同一类型，优先级：str>float>int

- 使用np的函数创建

  - np.ones(shape, dtype=None, order='C')

    ```
    np.ones([3,3])
    输出结果：
    array([[ 1.,  1.,  1.],
           [ 1.,  1.,  1.],
           [ 1.,  1.,  1.]])
    
    np.ones([3,3],dtype=int)
    输出结果：
    array([[1, 1, 1],
           [1, 1, 1],
           [1, 1, 1]])
    ```

    - 返回ndarray对象，元素都为1

    - shape代表维度
      shape=5 表示一维的5个元素
      shape=(2,5) 表示二维的2行5列

    - dtype 代表数据类型

      numpy.int/int8/float/float32...(numpy中类型)
      str/int/float...(python的数据类型)

    - order代表在内存中存储的顺序
      可选F或C
      F 多维数据组的列优先
      C多维数据组的行优先

  - np.full(shape, fill_value, dtype=None, order='C')   #fill_value 填充的内容

    ```
    np.full([3,3],3.14)
    输出结果：
    array([[ 3.14,  3.14,  3.14],
           [ 3.14,  3.14,  3.14],
           [ 3.14,  3.14,  3.14]])
    ```

  - np.eye(N, M=None, k=0, dtype=float)

    ```
    np.eye(4)
    输出结果：
    array([[ 1.,  0.,  0.,  0.],
           [ 0.,  1.,  0.,  0.],
           [ 0.,  0.,  1.,  0.],
           [ 0.,  0.,  0.,  1.]])
    
    
    np.eye(4，4，1)   对角线上方偏移1
    输出结果：
    array([[ 0.,  1.,  0.,  0.],
               [ 0.,  0.,  1.,  0.],
               [ 0.,  0.,  0.,  1.],
               [ 0.,  0.,  0.,  0.]])
    
    
    np.eye(4，4，-1)  对角线下方偏移1
    输出结果：
    array([[ 0.,  0.,  0.,  0.],
              [ 1.,  0.,  0.,  0.],
              [ 0.,  1.,  0.,  0.],
              [ 0.,  0.,  1.,  0.]])
    ```

    

    - n阶单位矩阵
      	对角线为1其他的位置为0
    - N表示行数
    - M表示列数， 未指定时为N
    - k=0表示按对角线填充1，>0时在对角线上方偏移 k,  <0时在对角线下方偏移 k
      				

  - np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)

    ```
    等差数列
    np.linspace(0,10,5)
    输出结果：
    array([  0. ,   2.5,   5. ,   7.5,  10. ])
    ```

    - 指定元素个数来生成一个等差数列
    - start 开始的数值
    - end 结束的数值
    - endpoint默认为True, 包含end的值，False则不包含end的值
    - num表示元数的个数
    - retstep默认为False，不显示等差间隔值，True显示间隔值
    - dtype 代表数据类型

  - np.arange([start, ]stop, [step, ]dtype=None)

    ```
    等差数列
    np.arange(0,10,2)
    输出结果：
    array([0, 2, 4, 6, 8])
    ```

    - 指定步长来创建一个等差数列
    - start 开始的数值,默认为0
    - step为步长，默认为1
      官方提示： 使用非int类型的步长时，如0.1结果可能不一致

  - np.random.random(size=None)

    - 生成0到1的随机小数，左闭右开[0.0, 1.0)
    - size为元素个数，可以是int或tuple类型，默认为1

  ```
  np.random.random(100)
  # 每次都不一样
  输出结果：
  array([ 0.01150584,  0.52951883,  0.07689008,  0.72856545,  0.26700953,
          0.38506149,  0.56252666,  0.59974406,  0.38050248,  0.14719008,
          0.6360734 ,  0.27812695,  0.73241298,  0.10904588,  0.57071762,
          0.56808218,  0.33192772,  0.61444518,  0.07289501,  0.86464595,
          0.71140253,  0.3221285 ,  0.92556313,  0.26511829,  0.8487166 ,
          0.38634413,  0.32169243,  0.80473196,  0.92050868,  0.17325157,
          0.63503329,  0.89463233,  0.02796505,  0.04396453,  0.20603116,
          0.77663591,  0.96595455,  0.77823865,  0.90867045,  0.39274922,
          0.89526325,  0.26002297,  0.38606984,  0.69176715,  0.3170825 ,
          0.86994578,  0.35648567,  0.19945661,  0.16109699,  0.58245076,
          0.20239367,  0.7099113 ,  0.41444565,  0.16725785,  0.01170234,
          0.79989105,  0.76490449,  0.25418521,  0.55082581,  0.29550998,
          0.02919009,  0.32737646,  0.29171893,  0.67664205,  0.24447834,
          0.49631976,  0.41136961,  0.82478264,  0.76439988,  0.78829201,
          0.24360075,  0.26151563,  0.51388418,  0.19823452,  0.44097815,
          0.53198973,  0.50187154,  0.72374522,  0.11090765,  0.63469357,
          0.69199977,  0.97093079,  0.35920669,  0.86493051,  0.01984456,
          0.32219702,  0.58608421,  0.26591245,  0.51851213,  0.7896492 ,
          0.04914308,  0.28711285,  0.36225247,  0.21299697,  0.99046025,
          0.11375325,  0.70964612,  0.06599185,  0.47323442,  0.62003386])
  
  //////////////////////////////////////////////////////////////////////////////////////////////////
  np.random.random([3,3])
  输出结果：
  array([[ 0.37590691,  0.15563239,  0.7754904 ],
         [ 0.40353019,  0.59708594,  0.57000741],
         [ 0.33286511,  0.15678606,  0.58814922]])
  ```

  - np.random.randint(low, high=None, size=None, dtype='l')

    - 生成[low, high)区间的随机整数

    - low 最小数值(包含)

    - high 最大数值(不包含)

      high 是None时，即是从[0, low) 之间取值

    - size为元素个数，可以是int或tuple类型，默认为1
      				

  - np.random.randn(d0, d1, ..., dn)

    - 从“标准正态”分布中返回一个样本

    - d0, d1,..dn 表示n阶维度

    - 扩展

      - np.random.seed(seed=None)  设置随机的种子

        指定种子之后，每次随机产生的数都是一样的

      - np.random.normal(loc=0.0, scale=1.0, size=None)

        - 从高斯分布抽取一个样本	

        ```
        loc和scale的说明：
        
        若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ，σ^2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。
         
        ```

        高斯分布

        ​		![image-20200116162536359](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200116162536359.png)

        标准正态分布

        ![image-20200116162743605](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200116162743605.png)

        - 可以控制期望值和方差变化
        - loc 期望值， 决定正态分布的位置
        - scale 方差 ，决定了分布的幅度
        - size 可以是int或tuple, 表示元数个数

### ndarray的属性

- ndim：维度 
- shape：形状（各维度的长度） 
- size：总长度
- dtype：元素类型

### ndarray的基本操作

- 索引

  ```
  一维与列表完全一致 多维时同理
  np.random.seed(1)
  x = np.random.randint(10,size=[3,4,5])
  print(x[2,0,0])
  print(x)
  
  5
  [[[5 8 9 5 0]
    [0 1 7 6 9]
    [2 4 5 2 4]
    [2 4 7 7 9]]
  
   [[1 7 0 6 9]
    [9 7 6 9 1]
    [0 1 8 8 3]
    [9 8 7 3 6]]
  
   [[5 1 9 3 4]
    [8 1 4 0 3]
    [9 2 0 4 9]
    [2 7 7 9 8]]]
  ```

  - eg1
    d = np.random.randint(1, 10, size=10)
    array([6, 5, 2, 7, 2, 5, 1, 1, 5, 6])
    d[0] = 6
    d[[0, 2, 3]] =  array([6, 2, 7])
  - eg2
    d = np.random.randint(1, 10, size=(2, 3))
    array([[1, 8, 4],[5, 6, 4]])
    d[0][0]= 1,  d[0,0] = 1

- 切片

  ```
  一维与列表完全一致 多维时同理
  np.random.seed(0)
  x = np.random.randint(100,size = (10,4))
  x
  
  输出结果：
  array([[44, 47, 64, 67],
         [67,  9, 83, 21],
         [36, 87, 70, 88],
         [88, 12, 58, 65],
         [39, 87, 46, 88],
         [81, 37, 25, 77],
         [72,  9, 20, 80],
         [69, 79, 47, 64],
         [82, 99, 88, 49],
         [29, 19, 19, 14]])
  
  行切片：
  x[7:10]
  切片结果：
  array([[69, 79, 47, 64],
         [82, 99, 88, 49],
         [29, 19, 19, 14]])
  
  列切片：
  	x[:2,  0:2 ]  
  结果：
  	array([[44, 47],
         [67,  9]])
        
  ```

  - 行切片：a[1: 2]  第2行
  - 列切片： a[:1, 1:2]  第1行，第2列

- 变形

  - reshape(size)函数
    注意参数是一个tuple， 另外变形前后的元素数量是一样的
    如， np.arange(0,16).reshape(4,4)
    np.arange(0, 20, step=2).reshape(-1,1)

  - 转置
    transpose()/T  实现行列转置

  - 级联

    - np.concatenate((a1, a2, ...), axis=0) 

    - 【重点】级联的方向默认是shape这个tuple的第一个值所代表的维度方向
      可通过axis参数改变级联的方向
      					注意
      					维度必须相同
      					形状相符

    - axis

      ```
      import numpy as np
      x = np.array([[[1,2,3],[2,2,3],[3,3,3]],[[4,4,4],[5,5,5],[6,6,6]]])
      print(x)
      print(x.shape)
      
      输出：
      [[[1 2 3]
        [2 2 3]
        [3 3 3]]
      
       [[4 4 4]
        [5 5 5]
        [6 6 6]]]
      (2, 3, 3)
      
      ////////////////////////////////
      w = np.concatenate([x,x],axis = 0)
      print(w.shape)
      print(w)
      输出：
      (4, 3, 3)
      [[[1 2 3]
        [2 2 3]
        [3 3 3]]
      
       [[4 4 4]
        [5 5 5]
        [6 6 6]]
      
       [[1 2 3]
        [2 2 3]
        [3 3 3]]
      
       [[4 4 4]
        [5 5 5]
        [6 6 6]]]
      ////////////////////////////////////
      w = np.concatenate([x,x],axis = 1)
      print(w.shape)
      print(w)
      输出：
      (2, 6, 3)
      [[[1 2 3]
        [2 2 3]
        [3 3 3]
        [1 2 3]
        [2 2 3]
        [3 3 3]]
      
       [[4 4 4]
        [5 5 5]
        [6 6 6]
        [4 4 4]
        [5 5 5]
        [6 6 6]]]
      /////////////////////////////////////
      w = np.concatenate([x,x],axis = 2)
      print(w.shape)
      print(w)
      
      输出：
      (2, 3, 6)
      [[[1 2 3 1 2 3]
        [2 2 3 2 2 3]
        [3 3 3 3 3 3]]
      
       [[4 4 4 4 4 4]
        [5 5 5 5 5 5]
        [6 6 6 6 6 6]]]
      ```

      可以理解维度方向

    - np.hstack(tuple)与np.vstack(tuple) 水平级联与垂直级联

      ```
      x = np.random.randint(1, 10, size=(2, 3))
      y = np.random.randint(20, 30, size= (2,3))
      print(x)
      print('*'*50)
      print(y)
      [[3 8 2]
       [1 8 1]]
      **************************************************
      [[29 25 21]
       [21 25 29]]
      
      np.vstack((x, y))  # 行关联（合并），垂直方向
      
      array([[ 3,  8,  2],
             [ 1,  8,  1],
             [29, 25, 21],
             [21, 25, 29]])
      
      **************************************************
      np.hstack((x, y))  # 列关联（合并）， 水平方向
      
      array([[ 3,  8,  2, 29, 25, 21],
             [ 1,  8,  1, 21, 25, 29]])
      ```

  - 切分

    ```
    与级联类似，三个函数完成切分工作：
    np.split
    np.vsplit
    np.hsplit
    ```

    - np.split(ary, indices_or_sections, axis=0)

      - indices_or_sections 可以是 int 或 范围

        ```
        x = np.arange(1,10)
        x
        
        输出：
        array([1, 2, 3, 4, 5, 6, 7, 8, 9])
        
        x1,x2,x3 = np.split(x,[3,5])
        print(x1,x2,x3)
        输出：
        [1 2 3] [4 5] [6 7 8 9]
        ```

        如果是int类型，必须是可以被总行或总列数整除，否则会报错
        eg
        x = np.arange(10)  # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
        np.split(x, [2,5])   # [array([0, 1]), array([2, 3, 4]), array([5, 6, 7, 8, 9])]

      - np.hsplit(ary, indices_or_sections)

        ```
        x = np.arange(16).reshape(4,4)
        x
        输出：
        array([[ 0,  1,  2,  3],
               [ 4,  5,  6,  7],
               [ 8,  9, 10, 11],
               [12, 13, 14, 15]])
        
        print(np.hsplit(x,[2,3]))
        输出：
        [array([[ 0,  1],[ 4,  5], [ 8,  9], [12, 13]]),
        		 array([[ 2],  [ 6],  [10], [14]]), 
        		 array([[ 3],  [ 7],  [11],  [15]])]
        ```

        

      - np.vsplit(ary, indices_or_sections)

        ```
        x = np.arange(16).reshape(4,4)  # 维度一般是2维 
        x
        输出：
        array([[ 0,  1,  2,  3],
               [ 4,  5,  6,  7],
               [ 8,  9, 10, 11],
               [12, 13, 14, 15]])
        
        print(np.vsplit(x,[2,3]))
        输出：
        [array([[0, 1, 2, 3],
                [4, 5, 6, 7]]), array([[ 8,  9, 10, 11]]), array([[12, 13, 14, 15]])]
        
        
        ```

  - 副本

    - 所有赋值运算不会为ndarray的任何元素创建副本。对赋值后的对象的操作也对原来的对象生效

      ```
      a = np.array([1,2,3])
      b=a
      print(a,b)
      
      输出：
      [1 2 3] [1 2 3]
      
      
      b[0]=2
      a
      输出：
      array([2, 2, 3])
      ```

    - 可使用copy()函数创建副本

      ```
      a = np.array([1,2,3])
      b = a.copy()
      b
      输出：
      [1,2,3]
      
      b[0]=3
      print(a,b)
      输出：
      [1 2 3] [3 2 3]
      ```

### ndarray的聚合操作

- 求和np.sum

  ```
  import numpy as np
  np.random.seed(0)
  a = np.random.randint(1000,size = 100)
  print(np.sum(a))
  
  b = np.random.randint(1000,size = (3,4,5))
  print(np.sum(b))
  
  输出：
  52397
  32865
  
  计算求和时间
  %timeit np.sum(a)
  
  输出：
  2.63 µs ± 34.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
  
  l = [1,2,3]
  np.sum(l)
  输出：
  ```

- 最大最小值：np.max/ np.min

  ```
  %time max(a)
  %time np.max(a)
  print(max(a))
  print(np.max(a))
  print(min(a))
  print(np.min(a))
  
  输出：
  CPU times: user 0 ns, sys: 0 ns, total: 0 ns
  Wall time: 22.4 µs
  CPU times: user 0 ns, sys: 0 ns, total: 0 ns
  Wall time: 61.3 µs
  999
  999
  9
  9
  ```

- 平均：np.mean

- 其他聚合操作

  ```
  Function Name    NaN-safe Version    Description
  np.sum    np.nansum    Compute sum of elements
  np.prod    np.nanprod    Compute product of elements
  np.mean    np.nanmean    Compute mean of elements
  np.std    np.nanstd    Compute standard deviation
  np.var    np.nanvar    Compute variance
  np.min    np.nanmin    Find minimum value
  np.max    np.nanmax    Find maximum value
  np.argmin    np.nanargmin    Find index of minimum value
  np.argmax    np.nanargmax    Find index of maximum value
  np.median    np.nanmedian    Compute median of elements
  np.percentile    np.nanpercentile    Compute rank-based statistics of elements
  np.any    N/A    Evaluate whether any elements are true
  np.all    N/A    Evaluate whether all elements are true
  
  ////////////////////////////////////////////////////
  a = np.array([1,2,3,4,np.nan])
  print(np.sum(a))
  print(np.nansum(a))
  
  输出：
  nan
  10.0
  ```

- 操作文件

  ```
  import pandas as pd
  data = pd.read_csv('../../data/president_heights.csv')
  heights = np.array(data['height(cm)'])
  heights
  
  输出：
  array([189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175,
         178, 183, 193, 178, 173, 174, 183, 183, 168, 170, 178, 182, 180,
         183, 178, 182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188,
         188, 182, 185])
  ```

  -  求平均值

    np.mean(heights)

  - 求最大值
    np.max(heights)

  - 求最小值
    np.min(heights)

  - 计算标准差

    ![image-20200116172128446](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200116172128446.png)

    ⒈方差s^2=[（x1-x）^2+（x2-x）^2+......（xn-x）^2]/（n）（x为平均数）
    ⒉标准差=方差的算术平方根

  ​        heights.std(heights)			

  ### ndarray的矩阵操作

  - 基本矩阵操作

    - 算术运算符：加减乘除

      ```
      a = np.array([[1,2,3],
      [4,5,6]])
      a
      输出：
      array([[1, 2, 3],
             [4, 5, 6]])
      a+1
      输出：
      array([[2, 3, 4],
             [5, 6, 7]])
      a*2
      输出：
      array([[ 2,  4,  6],
             [ 8, 10, 12]])
      
      
      a+[[1,4,9],[3,3,3]]
      输出：
      array([[ 2,  6, 12],
             [ 7,  8,  9]])
      
      
      a*2-2
      输出：
      array([[ 0,  2,  4],
             [ 6,  8, 10]])
      ```

      

    - ​	矩阵积
      ​					np.dot(A,B)

         ![image-20200116172440517](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200116172440517.png) 						

  - 广播机制

    【重要】ndarray广播机制的两条规则

    规则一：为缺失的维度补1
    规则二：假定缺失元素, 用已有值填充
    例1： m = np.ones((2, 3)) a = np.arange(3) 求m+a
    例2： a = np.arange(3).reshape((3, 1)) b = np.arange(3) 求a+b
    习题 a = np.ones((4, 1)) b = np.arange(4) 求a+b

    

### ndarray的排序

小测试

- 冒泡排序

```
m = np.ones((2,3))
m
输出：
array([[ 1.,  1.,  1.],
       [ 1.,  1.,  1.]])

a = np.arange(3)
a
输出：
array([0, 1, 2])

m+a
输出：
array([[ 1.,  2.,  3.],
       [ 1.,  2.,  3.]])
```

- 使用以上所学numpy的知识，对一个ndarray对象进行选择排序
  					np.argmin()
    					np.argmax()

  ​                    如果是选择的元素索引范围，则会重新以0开始的

- 快速排序
  np.sort()与ndarray.sort()都可以，但有区别：
  np.sort()不改变输入
  ndarray.sort()本地处理，不占用空间，但改变输入	

  ```
  def quick_sort(nums, start, end):
      #判断low是否小于high,如果为false,直接返回
      if start < end:
          i,j = start,end
          #设置基准数
          base = nums[i]
  
          while i < j:
              #如果列表后边的数,比基准数大或相等,则前移一位直到有比基准数小的数出现
              while (i < j) and (nums[j] >= base):
                  j = j - 1
  
              #如找到, 则把第j个元素赋值给第个元素i,此时表中i,j个元素相等
              nums[i] = nums[j]
  
              #同样的方式比较前半区
              while (i < j) and (nums[i] <= base):
                  i = i + 1
              nums[j] = nums[i]
  
          #做完第一轮比较之后,列表被分成了两个半区,并且i=j,需要将这个数设置回base
          nums[i] = base
  
          #递归前后半区
          quick_sort(nums, start, i - 1)
          quick_sort(nums, j + 1, end)
  
  
  ml = [49,38,65,97,76,13,27,49]
  print("Quick Sort: ")
  quick_sort(ml,0,len(ml)-1)
  print(ml)
  ```

   np.sort()不改变输入

  ```
  np.random.seed(10)
  a = np.random.randint(0,100,10)
  b = np.random.randint(0,100,10)
  print(a,b)
  print(np.sort(a),a)
  
  输入：
  [ 9 15 64 28 89 93 29  8 73  0] [40 36 16 11 54 88 62 33 72 78]
  [ 0  8  9 15 28 29 64 73 89 93] [ 9 15 64 28 89 93 29  8 73  0]
  ```

  ndarray.sort()本地处理，不占用空间，但改变输入

  ```
  np.random.seed(20)
  a = np.random.randint(0,100,10)
  b = np.random.randint(0,100,10)
  print(a,b)
  print(a.sort(),a)
  
  输出：
  [99 90 15 95 28 90  9 20 75 22] [71 34 96 40 85 90 26 83 16 62]
  None [ 9 15 20 22 28 75 90 90 95 99]
  ```

- 部分排序
  np.partition(a,k)
  有的时候我们不是对全部数据感兴趣，我们可能只对最小或最大的一部分感兴趣。
  当k为正时，我们想要得到最小的k个数
  当k为负时，我们想要得到最大的k个数
  	k为正

  ```
  import numpy as np
  a = np.random.randint(0,100,10)
  print(a)
  print(np.partition(a, 3))
  
  输出：
  [67  0 63 42 30 82 28 63 95 13]
  [ 0 13 28 30 42 82 63 63 95 67]
  
  ----------二维数组---------------
  a1 = np.array([[4,1,3,2], [8,0,10,12], [90, 70,100, 120], [86, 80, 79,84]])
  
  display(a1)
  
  display(np.partition(a1,2, axis=0) )   # 沿第一个轴排序， 列内排
  
  display(np.partition(a1,2, axis=1) )  ＃沿第二个轴排序，行内排
  
  array([[  4,   1,   3,   2],
         [  8,   0,  10,  12],
         [ 90,  70, 100, 120],
         [ 86,  80,  79,  84]])
  
  array([[  4,   0,   3,   2],
         [  8,   1,  10,  12],
         [ 86,  70,  79,  84],
         [ 90,  80, 100, 120]])
  
  array([[  1,   2,   3,   4],
         [  0,   8,  10,  12],
         [ 70,  90, 100, 120],
         [ 79,  80,  84,  86]])
  ```

  ​	k为负

  ```
  b = np.random.randint(0,100,10)
  print(b)
  print(np.partition(b,-3))
  
  输出：
  [89 66 11 58 97  7 50 13 87 77]
  [ 7 13 11 50 58 66 77 87 89 97]
  ```

# Day02-pandas-I

## opencv2之人脸识别

### 安装依赖库

参考：https://pypi.org/project/opencv-python/
pip install opencv_python i https://mirrors.aliyun.com/pypi/simple
only main modules
pip install opencv-contrib-python
main and contrib modules （ OpenCV 文档中扩展的）

### 导包

import cv2

### 加载图片

img = cv2.imread('.//xxxx.jpg')

### 重置图片大小

img2 = cv2.resize(img, (71,71))

### 加载算法

- face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')
  				当前文档相对的路径下存在此文件

- cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
  				cv2包中提供

- 算法对象认别人脸
  			face = face_cascade.detectMultiScale(img)

- 人脸位置填充新的图片
  	for (x,y,w,h) in face:
      img[y:y+w,x:x+h] = img2

- 显示图片

  import matplotlib.pyplot as plt
  plt.imshow(img[::,::,::-1])
  plt.show()

- 扩展

  - 画线 cv2.line()
    				img :图像，起点坐标，终点坐标，颜色，线的宽度
    				cv2.line(img, (10,10), (510,510), (0, 255,0),5)
  - 画方形 cv2.rectangle()
    				cv2.rectangle(img, (x, y), (x+width, y+height), (255, 0, 0), 2)
  - 写字符 cv2.putText()
    				font = cv2.FONT_HERSHEY_SIMPLEX
    				cv2.putText(img, ‘文本Hello’, (50,300),font,4,(255,0,255),2,cv2.LINE_A4)

## 什么是pandas

- Python Data Analysis Library 或 pandas 是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的
- pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具
- pandas提供了大量能使我们快速便捷地处理数据的函数和方法
- 它使Python成为强大而高效的数据分析环境的重要因素之一

## 导入

​		#pandas 源于 numpy 两个总是一起使用

​		import numpy as np
​		import pandas as pd
​		from pandas import Series,DataFrame

		[概要
			Series 是一个类似一维数组的数据结构
			DataFrame 数据帧
			类似于Excel,DataFrame组织数据，处理数据]
## Series

### Series创建

- 由列表或numpy数组创建
  默认索引为0到N-1的整数型索引

```
obj = Series([1,2,3,4])
print(obj)
输出：
0    1
1    2
2    3
3    4
dtype: int64
#还可以通过设置index参数指定索引
obj2 = Series([1,2,3,4],index=['a','b','c','d'])
obj2
输出：
a    1
b    2
c    3
d    4
dtype: int64
注意：由ndarray创建的是引用，而不是副本。对Series元素的改变也会改变原来的ndarray对象中的元素。（列表没有这种情况）
a = np.array([1,2,3])
obj = Series(a)
obj
输出：
0    1
1    2
2    3
dtype: int64
obj[0]=0
print(a)
print(obj)
输出：
[0 2 3]
0    0
1    2
2    3
dtype: int64
```

- - Series有两个核心属性: values和index
    - values 为数据,  是ndarray类型
    - index为索引，默认为int, 也可以指定字母或名称
  - obj = Series([1,2,3,4])
  - obj = Series(np.array([1,2,3,4]))
  - obj = Series(np.array([1,2,3,4]), index=['A', 'B', 'C', 'D'])

- 由字典创建

  ```
  obj = Series({'a':1,'b':2})
  obj
  输出：
  a    1
  b    2
  dtype: int64
  ```

  - obj = Series({'a':1,'b':2})
  - 注意： 由字典生成的Series是有序的
    obj[0]  输出： 1

### Series索引和切片

​	(1) 显式索引：

- 使用index中的元素作为索引值

   - 使用.loc[]（推荐）
       注意: 此时是闭区间

       ```
       obj = Series({'a':10,'b':12,'c':17})
       obj.loc['a']
       a    10
       b    12
       dtype: int64
       obj['a']
       输出：
       10
       obj['a':'c']
       输出：
       a    10
       b    12
       c    17
       dtype: int64
       ```

​		(2) 隐式索引：

- 使用整数作为索引值

- 使用.iloc[]（推荐）
注意: 此时是半开区间
	
```
obj[0:1]
输出
	a    10
	dtype: int64
obj.iloc[0]
	输出：
	10
	obj.iloc[0:1]
	输出：
a    10
	dtype: int64
```

			练习2：
	使用多种方法对练习1创建的Series sss进行索引和切片：
	索引： 数学 150
	切片： 语文 150 数学 150 英语 150

### Series的基本概念

	- 可以把Series看成一个定长的有序字典
	
	- 可以通过shape，size，index,values等得到series的属性
	
	- 可以通过head(),tail()快速查看Series对象的样式
	  	head() 前几条数据
	  	tail() 后几条数据
	
	- 索引对齐运算
	  当索引没有对应的值时，可能出现缺失数据显示NaN（not a number）的情况
	  	np.nan 即为NaN
	  	np.NAN
	  	np.NaN
	
	- 可以使用pd.isnull()，pd.notnull()，或自带isnull(),notnull()函数检测缺失数据
	
	  ```
	  obj = Series([10,4,np.nan])
	  #判断Series是否为不为null，为null返回false
	  notnull = pd.notnull(obj)
	  #如果为false将空值设为0
	  for i,d in enumerate(notnull):
	      if d ==0:
	          obj[i] = 0
	  print(obj)
	  ```
	
	- Series对象本身及其索引都有一个name属性
	
	  ```
	  obj.name='123'
	  print(obj)
	  Series.name = "Hello World"
	  print(Series.name)
	  输出：
	  a    1.0
	  b    2.0
	  d    NaN
	  Name: 123, dtype: float64
	  Hello World
	  ```

### Series的运算

- 适用于numpy的数组运算也适用于Series

- Series之间的运算，在运算中自动对齐不同索引的数据
  如果索引不对应，则补NaN

  ```
  A = pd.Series([2,4,6], index=[0,1,2])
  B = pd.Series([1,3,5], index=[1,2,3])
  display(A,B)
  输出：
  0    2
  1    4
  2    6
  dtype: int64
  1    1
  2    3
  3    5
  dtype: int64
  A+B
  输出：
  0    NaN
  1    5.0
  2    9.0
  3    NaN
  dtype: float64
  ```

  - 注意：要想保留所有的index，则需要使用.add()函数
    				fill_value 填充NaN的指定值
  - 扩展
    				add() 加法
    				sub() 减法
    				mul() 乘法
    				div() 除法

## DataFrame

### DataFrame创建

最常用的方法是传递一个字典来创建。
DataFrame以字典的键作为每一【列】的名称，以字典的值（一个数组）作为每一列。
此外，DataFrame会自动加上每一行的索引（和Series一样）。
同Series一样，若传入的列与字典的键不匹配，则相应的值为NaN。

### DataFrame的索引

- 对列进行索引
  		df[column]
    		df[[column1,column2]]
    		df.loc[:,colum1:column2]

  ```
  对列进行索引
  - 通过类似字典的方式
  - 通过属性的方式
  可以将DataFrame的列获取为一个Series。返回的Series拥有原DataFrame相同的索引，且name属性也已经设置好了，就是相应的列名。
  print(frame)
  输出：
          color  object  price weight
  one      blue    ball    1.2    NaN
  two     green     pen    1.0    NaN
  three  yellow  pencil    0.6    NaN
  four      red   paper    0.9    NaN
  five    white     mug    1.7    NaN
  frame['color']
  Out[14]:
  one        blue
  two       green
  three    yellow
  four        red
  five      white
  Name: color, dtype: object
  方式二：frame.color
  输出：
  one        blue
  two       green
  three    yellow
  four        red
  five      white
  Name: color, dtype: object
  ```

  ​			

- 对行进行索引

  - 使用.ix[]来进行行索引
    		之前的函数（已过时），同loc[]
  - 使用.loc[]加index来进行行索引
    			df.loc[index]
          			df.loc[[index1,index2]]
          			df.loc[index1:index2]
          			df[index1:index2]
  - 使用.iloc[]加整数来进行行索引
    			df.iloc[0]
          			df.iloc[[0,1]]
          			df.iloc[0:1]

```
2) 对行进行索引
- 使用.ix[]来进行行索引
- 使用.loc[]加index来进行行索引
- 使用.iloc[]加整数来进行行索引
同样返回一个Series，index为原来的columns。
frame.ix['one']
输出：
color     blue
object    ball
price      1.2
weight     NaN
Name: one, dtype: object
type(frame.ix['one'])
输出：
Out[52]:
pandas.core.series.Series
frame.loc["two"]
输出：
color     green
object      pen
price         1
weight      NaN
Name: two, dtype: object
//////////////////////////////////////////////////////////
print(frame)
frame.iloc[0:10]
输出：
        color  object  price weight
one      blue    ball    1.2    NaN
two     green     pen    1.0    NaN
three  yellow  pencil    0.6    NaN
four      red   paper    0.9    NaN
five    white     mug    1.7    NaN
Out[41]:
		color		object	price	weight
one	blue		ball		1.2	NaN
two	green	pen		1.0	NaN
three	yellow	pencil	0.6	NaN
four	red		paper	0.9	NaN
five	white		mug		1.7	NaN
```



- 对元素索引的方法
  		df.loc['行标签‘，’列标签']

  ```
  对元素索引的方法
  - 先使用列索引
  - 先使用行索引
  - 使用values属性（二维numpy数组）
  print(frame)
  print("使用列索引")
  print(frame['color']['one'])
  print(frame.color['one'])
  print("使用行索引")
  print(frame.ix['one']['color'])
  print(frame.loc['one','color'])
  print(frame.iloc[0][0:2])
  print("使用values属性")
  print(frame.values[[0][0]])
  print(frame.values[0][1:3])
  输出：
          color  object  price weight
  one      blue    ball    1.2    NaN
  two     green     pen    1.0    NaN
  three  yellow  pencil    0.6    NaN
  four      red   paper    0.9    NaN
  five    white     mug    1.7    NaN
  使用列索引
  blue
  blue
  使用行索引
  blue
  blue
  color     blue
  object    ball
  Name: one, dtype: object
  使用values属性
  ['blue' 'ball' 1.2 nan]
  ['ball' 1.2]
  ```

  - 【注意】 直接用中括号[ ]时：
    索引表示的是列索引
    切片表示的是行切片

    ```
    这是列索引
    print(frame['color'])
    输出：
    one        blue
    two       green
    three    yellow
    four        red
    five      white
    Name: color, dtype: object
    使用切片--------->对应行
    frame['one':'two']
    输出：
    	color	object	price	weight
    one	blue	ball	1.2	NaN
    two	green	pen	1.0	NaN
    ```

    

### DataFrame的运算

​	DataFrame之间的运算

```
同Series一样：
在运算中自动对齐不同索引的数据
如果索引不对应，则补NaN

A = DataFrame(np.random.randint(0,20,(2,2)),columns = list('ab'))
A
输出：
	a		b
0	1		1
1	15	10

B = DataFrame(np.random.randint(0,10,(3,3)),columns = list('abc'))
B

输出：
	a	b	c
0	6	3	8
1	2	7	4
2	6	7	4

A+B
输出：
		a			b			c
0		14.0		12.0		NaN
1		6.0		14.0		NaN
2		NaN	NaN	NaN

A.add(B,fill_value=0)

输出：
	a			b			c
0	14.0		12.0		8.0
1	6.0		14.0		4.0
2	6.0		7.0		4.0

转换成int类型数据：
A.add(B,fill_value=0).astype('int')

输出：

	a		b		c
0	10	4		8
1	15	10	3
2	8		4		4
```

![image-20200116192851325](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200116192851325.png)

Series与DataFrame之间的运算

```
【重要】
使用Python操作符：以行为单位操作（参数必须是行），对所有行都有效。
（类似于numpy中二维数组与一维数组的运算，但可能出现NaN）

使用pandas操作函数：
  axis=0：以列为单位操作（参数必须是列），对所有列都有效。
  axis=1：以行为单位操作（参数必须是行），对所有列都有效。

A = np.random.randint(10,size = (3,4))
A
输出：
array([[6, 8, 6, 2],
       [0, 4, 9, 3],
       [6, 9, 6, 5]])

B = np.random.randint(10,size = (4))
B

输出：
array([1, 9, 6, 1])


A-B
输出：
array([[ 5, -1,  0,  1],
       [-1, -5,  3,  2],
       [ 5,  0,  0,  4]])
```

​	DataFrame操作

```
A = np.random.randint(10,size = (3,4))
df = DataFrame(A, columns = list("QWER"))
df

输出：
	 Q		W	E		R
0	 7		 	9		3		5
1	 2			4		7		6
2	 8			8		1		6


df.iloc[0,::3] 

输出：
Q    7
R    5
Name: 0, dtype: int64

df - df.iloc[0,::2]

输出：
	  E			Q			  R				W
0	 0.0		 0.0		    NaN		  NaN
1	 4.0		 -5.0		NaN		 NaN
2	-2.0		 1.0		    NaN		 NaN
```

​		axis=0：以列为单位操作（参数必须是列），对所有列都有效。

```
#   axis=0：以列为单位操作（参数必须是列），对所有列都有效。
display(df)
display(df['Q'])
df.sub(df['Q'],axis = 0)
输出：
 	Q		W	E		R
0	7		9		3		5
1	2		4		7		6
2	8		8		1		6

0    7
1    2
2    8
Name: Q, dtype: int64

	Q		W	E		R
0	0		2		-4	-2
1	0		2		5		4
2	0		0		-7	-2
```

​	axis=1：以行为单位操作（参数必须是行），对所有行都有效

```
#   axis=1：以行为单位操作（参数必须是行），对所有列都有效
display(df)
display(df.iloc[0:2])
df.sub(df.iloc[0,::2],axis = 1)
输出：
    Q	W	E	R
0	7	9	3	5
1	2	4	7	6
2	8	8	1	6

   Q	W	 E	R
0	7	9	3	5
1	2	4	7	6

Out[143]:

	E		Q		R		W
0	0.0	0.0	NaN	NaN
1	4.0	-5.0	NaN	NaN
2	-2.0	1.0	NaN	NaN
```



## DataFrame处理丢失数据

### 有两种丢失数据

- None

  ```
  None是Python自带的，其类型为python object。因此，None不能参与到任何计算中
  object类型的运算要比int类型的运算慢得多
  %timeit sum_int = np.arange(1E6,dtype=int).sum()
  %timeit sum_int = np.arange(1E6,dtype=float).sum()
  %timeit sum_int = np.arange(1E6,dtype=object).sum()
  ```

  

- np.nan(NaN)

  ```
  np.nan是浮点类型，能参与到计算中。但计算的结果总是NaN
  ```

  

- np.NaN

- np.nan

- np.NAN

### pandas中的None与NaN

- pandas中None与np.nan都视作np.nan

  ```
  a = Series([1,np.nan,2,None])
  a
  输出：
  0    1.0
  1    NaN
  2    2.0
  3    NaN
  dtype: float64
  ```

- pandas中None与np.nan的操作

  - isnull()

  - notnull()

    ```
    data = Series([1,np.nan,'hello',None])
    data
    输出：
    0        1
    1      NaN
    2    hello
    3     None
    dtype: object
    
    
    data.isnull()
    输出：
    0    False
    1     True
    2    False
    3     True
    dtype: bool
    
    data[data.notnull()]
    输出：
    0        1
    2    hello
    dtype: object
    ```

    

  - dropna(): 过滤丢失数据
    			可以选择过滤的是行还是列（默认为行）
          			也可以选择过滤的方式

    ```
    df[3]=np.nan
    display(df)
    df.dropna(axis='index',how='all')
    输出：
    			昨天		今天		明天	3
    吃饭		1.0		NaN	2	NaN
    睡觉		2.0		3.0		7	NaN
    过家家		NaN	4.0	6	NaN
    小桥流水人家	314.0	299.0	1024	NaN
    Out[44]:
    昨天	今天	明天	3
    吃饭	1.0	NaN	2	NaN
    睡觉	2.0	3.0	7	NaN
    过家家	NaN	4.0	6	NaN
    小桥流水人家	314.0	299.0	1024	NaN
    ```

    

  - fillna(): 填充丢失数据

    ```
    data = Series([1,np.nan,2,None,4],index = list('abcdf'))
    data
    输出：
    a    1.0
    b    NaN
    c    2.0
    d    NaN
    f    4.0
    dtype: float64
    
    
    
    data.fillna(10)
    输出：
    a     1.0
    b    10.0
    c     2.0
    d    10.0
    e     3.0
    dtype: float64
    ```

    前置填充

    ```
    '''method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None
        Method to use for filling holes in reindexed Series
        pad / ffill: propagate last valid observation forward to next valid
        backfill / bfill: use NEXT valid observation to fill gap'''
    display(data)
    data.fillna(method='ffill')
    输出：
    a    1.0
    b    NaN
    c    2.0
    d    NaN
    f    4.0
    dtype: float64
    Out[60]:
    a    1.0
    b    1.0
    c    2.0
    d    2.0
    f    4.0
    dtype: float64
    ```

    axis = 0 行

    ```
    display(df)
    df.fillna(method='ffill',axis=0)#行
    输出：
    昨天	今天	明天	3
    吃饭	1.0	NaN	2	NaN
    睡觉	2.0	3.0	7	NaN
    过家家	NaN	4.0	6	NaN
    小桥流水人家	314.0	299.0	1024	NaN
    
    
    Out[68]:
           昨天	今天	明天	3
    吃饭	1.0	NaN	2	NaN
    睡觉	2.0	3.0	7	NaN
    过家家	2.0	4.0	6	NaN
    小桥流水人家	314.0	299.0	1024	NaN
    ```

    后置填充

    ```
    data.fillna(method='bfill')
    输出：
    Out[64]:
    a    1.0
    b    2.0
    c    2.0
    d    4.0
    f    4.0
    dtype: float64
    ```

    axis = 1 列

    ```
    import numpy as np
    import pandas as pd
    from pandas import Series,DataFrame
    df = DataFrame(index = ['吃饭','睡觉','写代码','人生'],columns = ["今天","昨天","明天"],
                   data = np.random.randint(0,1024,(4,3)),dtype="object")
    display(df)
    print(df.loc['睡觉'])
    df.loc["睡觉"][::2] =np.nan
    df['今天'][::2] = np.nan
    display(df)
    df.fillna(method='bfill',axis=1)#列
    输出：
    		今天	 昨天	明天
    吃饭	673	 893	436
    睡觉	632	 660	783
    写代码	783	112	89
    人生	1020	619	698
    
    今天    632
    昨天    660
    明天    783
    Name: 睡觉, dtype: object
    
    	今天	昨天	明天
    吃饭	NaN	893	436
    睡觉	NaN	660	NaN
    写代码	NaN	112	89
    人生	1020	619	698
    
    Out[39]:
    		今天	 	昨天		明天
    吃饭	893		893		436.0
    睡觉	660		660		NaN
    写代码	112	112		89.0
    人生	1020	619		698.0
    ```

    

  - 对于DataFrame来说，还要选择填充的轴axis。记住，对于DataFrame来说：
    axis=0：index/行
    axis=1：columns/列

### 函数

- fillna()
  - fill_value【全部填充】
  - method
    - pad/ffill      前置填充
    - backfill/bfill      后置填充
    
  - axis
    			0 行
          			1 列
  - limit
    			限制填充次数
- dropna()
  - how
    any   只要出现Nan则删除
    all	整行或列全部为Nan，则删除
  - inplace
    默认为False, 不修改原数据，True 修改原数据
  - axis
- 作业

1. psutil库
	cpu信息
	内存信息

# Day03-pandas-II

## 	pandas层次化索引

### 创建多层行索引

- 隐式构造	

```
df = DataFrame(np.random.randn(4,2),
               index=[['a','a','b','b'], [1,2,1,2]],
              columns=['data1','data2'])
df

输出：
		  data1		  data2
a	1	0.495897	0.130101
	2	0.551563	0.450209
b	1	0.664648	0.701401
	2	0.442188	0.500506
```

Series也可以创建多层索引

```
s = Series(np.random.rand(4),
                  index = [['a','a','b','b'],[1,2,1,2]])
s

输出：
a  1    0.192162
    2    0.236828
b  1    0.560971
    2    0.751398
dtype: float64
```

- 显示构造MultiIndex

  - 创建MultiIndex

    - 使用数组

      ```
      Mindex = pd.MultiIndex.from_arrays([['a','a','b','b'],[1,2,1,2]])
      Mindex
      
      输出：
      MultiIndex(levels=[['a', 'b'], [1, 2]],
                 labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
      ```

    - 使用tuple

      ```
      Mindex = pd.MultiIndex.from_tuples([('a',1),('a',2),('b',1),('b',2)])
      Mindex
      
      输出：
      MultiIndex(levels=[['a', 'b'], [1, 2]],
                 labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
      ```

    - 使用product

      ```
      Mindex = pd.MultiIndex.from_product([['a','b'],[1,2]])
      Mindex
      
      输出：
      MultiIndex(levels=[['a', 'b'], [1, 2]],
                 labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
      ```

  - 手机销售表

    ```
    # 创建各个城市（西安、北京）手机(Vivo, Oppo, Huiwei, MI)的
    # 每年（2017，2018）的四个季度(1,2,3,4)的销量情况
    
    index = MultiIndex.from_product([['西安', '北京'],
                                     ['Vivo', 'Oppo', 'Huiwei', 'MI']])
    column = MultiIndex.from_product([['2017年', '2018年'],
                                      ['1', '2', '3', '4']])
    data = np.random.randint(1, 100, size=(8, 8))
    
    phone_sales = DataFrame(data, index, column)
    phone_sales
    
    输出：
     				2017年 	     				2018年
    					1 		2 		3 		4 		1 		2 		3 		4
    西安 	Vivo 	34 	95 	52 	16 	33 	87 	85 	52
            Oppo 	19 	41 	69 	13 	21 	58 	4 		31
    	 Huiwei 	5 		84 	69 	38 	75 	82 	95 	30
          MI 			24 	27 	21 	41 	10 	10 	12 	11
    北京 	Vivo 	31 	10 	49 	75 	13 	15 	95 	17
            Oppo 	91 	77 	26 	97 	60 	91 	61 	26
         Huiwei 	42 	4 		79 	82 	29 	73 	90 	63
            MI 		87 	10 	63 	55 	27 	69 	6 		34
    ```

#### 多层列索引

除了行索引index，列索引columns也能用同样的方法创建多层索引

```
a = np.random.randint(0,150,(2,8))
Mindex = pd.MultiIndex.from_product([['语文','数学','英语','理综'],['期中','期末']])
ddd = DataFrame(data = a,
                index = ['张三','李四'],
                columns = Mindex)
ddd

输出：
语文	数学	英语	理综
期中	期末	期中	期末	期中	期末	期中	期末
张三	57	110	65	3	127	66	146	32
李四	73	24	57	19	35	133	57	127
```

#### 多层索引对象的索引与切片操作

- Series的操作

  【重要】对于Series来说，直接中括号[]与使用.loc()完全一样，因此，推荐使用中括号索引和切片

```
index = [('California',2000),('California',2010),
        ('New York',2000),('New York',2010),
        ('Texas',2000),('Texas',2010)]
index = [['California','California','New York','New York','Texas','Texas'],[2000,2010,2000,2010,2000,2010]]
# index = pd.MultiIndex.from_product([['California','New York','Texas'],[2000,2010]]) #相同的效果
populations = [33871648,37253956,
              18976457,19378102,
              20851820,25145561]
pop = Series(populations,index = index)
pop

输出：
California  2000    33871648
            2010    37253956
New York    2000    18976457
            2010    19378102
Texas       2000    20851820
            2010    25145561
dtype: int64
```

​			  索引

```
pop['California']

输出：
2000    33871648
2010    37253956
dtype: int64
```

​			 切片

```
pop['California':'New York']

输出：
California  2000    33871648
            2010    37253956
New York    2000    18976457
            2010    19378102
dtype: int64


pop[::-1]

输出：
Texas       2010    25145561
            2000    20851820
New York    2010    19378102
            2000    18976457
California  2010    37253956
            2000    33871648
dtype: int64


pop[:,2010]

输出：
California    37253956
New York      19378102
Texas         25145561
dtype: int64
```

- DataFrame的操作

  > 可以直接使用列名称来进行列索引

  ```
  index = [('California',2000),('California',2010),
          ('New York',2000),('New York',2010),
          ('Texas',2000),('Texas',2010)]
  populations = [33871648,37253956,
                18976457,19378102,
                20851820,25145561]
  Mindex = pd.MultiIndex.from_product([['California','New York','Texas'],[2000,2010]])
  pop = DataFrame(populations,index=Mindex,columns=['population'])
  pop
  
  输出：
  population
  California	2000	33871648
  2010	37253956
  New York	2000	18976457
  2010	19378102
  Texas	2000	20851820
  2010	25145561
  
  pop['population']
  
  输出：
  California  2000    33871648
              2010    37253956
  New York    2000    18976457
              2010    19378102
  Texas       2000    20851820
              2010    25145561
  Name: population, dtype: int64
  ```

  - phone_sales['2017年']
  - phone_sales[('2017年', '1')]
  - phone_sales[['2017年', '2018年']]

  > 使用行索引需要用iloc()，loc()等函数

  【极其重要】推荐使用loc()函数

  ```
  pop.loc['California']
  
  输出：
  population
  2000	33871648
  2010	37253956
  
  pop.loc[('California',2000)]
  
  输出：
  population    33871648
  Name: (California, 2000), dtype: int64
  
  pop.loc['California',2000]
  
  输出：
  population    33871648
  Name: (California, 2000), dtype: int64
  
  pop.loc['California',2000][0]
  
  输出：
  33871648
  
  pop.loc['California'].loc[2000]
  
  输出：
  population    33871648
  Name: 2000, dtype: int64
  
  pop.loc[('California',2000),'population']
  
  输出：
  33871648
  ```

  

  - phone_sales.loc['西安']

  - phone_sales.loc[('西安', 'Oppo')]

  - phone_sales.loc[['西安', '上海'], '2017年']

  - phone_sales.iloc[:2, :5]

    > 注意

    ```
    pop.loc['California':'New York',2000]
    输出：
    TypeError: cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [2000] of <class 'int'>
    ```

    在对行索引操作时，若一级行索引还有多个，对二级行索引会遇到问题！
    也就是说，无法直接对二级索引进行索引，必须让二级索引变成一级索引后才能对其进行索引！
    							

#### 索引的堆（unstack）

- stack()	把列索引转为行索引

```
populations = [33871648,37253956,
              18976457,19378102,
              20851820,25145561]
Mindex = pd.MultiIndex.from_product([['California','New York','Texas'],[2000,2010]])
pop = DataFrame(populations,index=Mindex,columns=['population'])
pop

输出：
							         population
California	2000		33871648
				     2010		37253956
New York	2000		18976457
				     2010		19378102
Texas		     2000		20851820
				     2010		25145561

///////////////////////////////////////////////////////////////////////////////
pop.unstack(level=0)

输出：
		population
		California		New York		Texas
2000	33871648		18976457		20851820
2010	37253956		19378102		25145561

pop.unstack(level=1)

输出：
					population
						2000				2010
California		33871648		37253956
New York		18976457		19378102
Texas			20851820		25145561
```

- unstack()
   把行索引转列索引
  【小技巧】使用unstack()的时候，level等于哪一个，哪一个就消失，出现在列里
   【注: level 是控制要转换的索引(索引最外层为0 ， 往里一层递增1)】

#### 聚合操作

【注意】
mean() /max()/min()/sum() 需要指定level
【小技巧】和unstack()相反，聚合的时候，level等于哪一个，哪一个就保留

## pandas拼接操作

- 级联：pd.concat, pd.append

  - 回顾numpy的级联
    axis = 0 行

    ```
    x = np.random.randn(3,3)
    x
    
    输出：
    array([[-0.09120671, -0.52299759, -0.16349379],
           [ 0.36666059, -1.3587184 ,  0.81883706],
           [ 1.0621649 ,  0.72906367, -0.41325065]])
    
    np.concatenate([x,x])
    
    输出：
    array([[-0.09120671, -0.52299759, -0.16349379],
           [ 0.36666059, -1.3587184 ,  0.81883706],
           [ 1.0621649 ,  0.72906367, -0.41325065],
           [-0.09120671, -0.52299759, -0.16349379],
           [ 0.36666059, -1.3587184 ,  0.81883706],
           [ 1.0621649 ,  0.72906367, -0.41325065]])
    ```

    axis = 1 列

    ```
    np.concatenate([x,x],axis=1)
    
    输出：
    array([[-0.09120671, -0.52299759, -0.16349379, -0.09120671, -0.52299759,
            -0.16349379],
           [ 0.36666059, -1.3587184 ,  0.81883706,  0.36666059, -1.3587184 ,
             0.81883706],
           [ 1.0621649 ,  0.72906367, -0.41325065,  1.0621649 ,  0.72906367,
            -0.41325065]])
    ```

  - 创建生成DataFrame的函数

    ```
    def make_df(cols,ind):
        '''
        快速生成一个DataFrame
        '''
        data = {c:[str(c)+str(i) for i in ind] for c in cols}
        return DataFrame(data,ind)
    
    
    调用：
    make_df('ABC', range(3))
    
    输出：
    
         A	    B	  C
    0	A0 	B0	  C0
    1	A1	B1	  C1
    2	A2 	B2	  C2
    ```

    

  - 使用pd.concat()级联

    ```
    pandas使用pd.concat函数，与np.concatenate函数类似，只是多了一些参数：
    pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,
              keys=None, levels=None, names=None, verify_integrity=False,
              copy=True)
    ```
  
    - 简单级联
  
    - 可以通过设置axis来改变级联方向
  
      ```
        df3 = make_df('WX', [1,3])
        df4 = make_df('QT', [5,8])
        display(df3,df4,pd.concat([df3,df4],axis=1))
        
        输出：
        
        		W	X
        1	W1	X1
        3	W3	X3
        		Q	T
        5	Q5	T5
        8	Q8	T8
        		W	X	Q	T
        1	W1	X1	NaN	NaN
        3	W3	X3	NaN	NaN
        5	NaN	NaN	Q5	T5
        8	NaN	NaN	Q8	T8
      ```
  
        - 默认0，纵向，即行合并
        - 1, 横向， 即列合并
  
      - 注意index在级联时可以重复
  
        - 横向级联时， ignore_index为True时，可以重新分配标签
  
        - 慎重使用，如果标签具有意义时，不能使用ignore_index
  
          ```
          x = make_df('AB',[0,1])
          y = make_df('AB',[1,8])
          pd.concat([x,y])
          
          输出：
               A	     B
          0	A0	 B0
          1	A1 	 B1
          1	A1  	 B1
          8	A8	 B8
          
          可以选择忽略index，重新索引
          x = make_df('AB',[0,1])
          y = make_df('AB',[1,8])
          pd.concat([x,y],ignore_index=True)
          
          输出：
               A	     B
          0	A0	 B0
          1	A1	 B1
          2	A4	 B4
          3	A8	 B8
          ```
  
      - 使用多层索引
  
        - keys 指定多层索引，不会改变原有的标签
  
    - 不匹配级联
  
      - 外连接：补NaN（默认模式）
  
        ```
        x = make_df('ABC',[1,2])
        y = make_df('BCD',[3,4])
        display(x,y,pd.concat([x,y]))
        
        输出：
              A	      B	      C
        1	A1	B1	C1
        2	A2	B2	C2
        
             B	     C	     D
        3	B3	C3	D3
        4	B4	C4	D4
        
             A	     B	     C	      D
        1	A1	B1	C1	NaN
        2	A2	B2	C2	NaN
        3	NaN	B3	C3	D3
        4	NaN	B4	C4	D4
        ```
  
        - df.fillna(value=0)
  
        - 练习： 查看表中哪一列存在空值NaN
  
          - df.isnull()
          - df.isnull().any() 快速判断某一行或某一列是否存在空值
          - df.loc[:, df.isnull().any()]
          - df.isnull().any(axis=1) 行中是否存在空值
  
        - 练习: 提取存在空值的行或列
  
          ```
          a1 = np.array([1,2,3,4,5])
          nan_list = [True, False, True, False, True]
          a1[nan_list]
          
          
          结果：
          array([1, 3, 5])
          
          
          ```
  
          - df[df.isnull().any(axis=1)]
  
      - 内连接(join="inner")：只连接匹配的项
  
        ```
        x = make_df('ABC',[1,2])
        y = make_df('BCD',[3,4])
        display(x,y,pd.concat([x,y],join="inner"))
        输出：
        		A		B		C
        1		A1	B1		C1
        2		A2	B2		C2
        
        		B		C		D
        3		B3		C3		D3
        4		B4		C4		D4
        
        	B		C
        1	B1		C1
        2	B2		C2
        3	B3		C3
        4	B4		C4
        ```
  
      - 连接指定轴 join_axes
  
        ```
        x = create_df('ABC', [1,2])
        y = create_df('BCD',[3,4])
        display(x,y,pd.concat([x,y],  join_axes=[x.columns]))
        
        输出：
        	A	     B	    C
        1	A1	B1	    C1
        2	A2	B2 	C2
        
        	B	  C	D
        3	B3	 C3	D3
        4	B4	 C4	D4
        
        	A	    B	      C
        1	A1	B1	 C1
        2	A2	B2	 C2
        3	NaN	B3	 C3
        4	NaN	B4	 C4
        ```
  
        - 扩展： join_axes=[pd.Index(['3', '4'])]
        
          ```
           d1 = new_df(list('1234'), list('ABCD'))
              d2 = new_df(list('345'), list('CD'))
        
              d3 = pd.concat((d1, d2), axis=1, join_axes=[pd.Index(['3', '4'])])
          ```
          
    
  - 使用pd.appnd()级联
  
    ```
    由于在后面级联的使用非常普遍，因此有一个函数append专门用于在后面添加
    
    
    x = make_df('AB',np.random.randint(10,size = 2))
    y = make_df('AB',np.random.randint(10,20,size = 2))
    display(x.append(y))
    
    输出：
    
    	  A		     B
    6	  A6		  B6
    8	  A8		  B8
    11  A11	      B11
    18	 A18	      B18
    ```

- 合并：pd.merge

  - 使用pd.merge()合并

    ```
    merge与concat的区别在于，merge需要依据某一共同的行或列来进行合并
    使用pd.merge()合并时，会自动根据两者相同column名称的那一列，作为key来进行合并。
    注意每一列元素的顺序不要求一致
    ```

    - 一对一合并

      ```
      import numpy as np
      import pandas as pd
      from pandas import Series,DataFrame
      df1 = DataFrame({'employee':['Bob','Jake','Lisa'],
                      'group':['Accounting','Engineering','Engineering'],
                      })
      df2 = DataFrame({'employee':['Lisa','Bob','Jake'],
                      'hire_date':[2004,2008,2012],
                      })
      display(df1,df2)
      
      输出：
      
       	employee	group
      0	Bob	Accounting
      1	Jake	Engineering
      2	Lisa	Engineering
      
      
      	employee	hire_date
      0	Lisa	2004
      1	Bob	2008
      2	Jake	2012
      
      
      df3 = pd.merge(df1,df2)
      df3
      
      输出：
      	employee	group	hire_date
      0	Bob	Accounting	2008
      1	Jake	Engineering	2012
      2	Lisa	Engineering	2004
      ```

      ```
      	employee	group
      0	Bob	Accounting
      1	Jake	Engineering
      2	Lisa	Engineering
      ​	employee	hire_date
      0	Lisa		2004
      1	Bob		2008
      2	Jake		2012
      ```

      - 概要
        ​	employee 员工姓名
        ​	group 分组名
        ​	hire_date 聘用年份

    - 一对多合并	

      ```
      employee	group		hire_date
      0	Lisa		Accounting		2004
      2	Jake		Engineering	2016
      ​	group			supervisor
      0		Accounting			Carly
      1		Engineering		Guido
      2		Engineering		Steve				supervisor 监管人
      ```

    - 多对多合并

      ```
      df1 = DataFrame({'employee':['Bob','Jake','Lisa'],
                       'group':['Accounting','Engineering','Engineering']})
      df5 = DataFrame({'group':['Engineering','Engineering','HR'],
                      'supervisor':['Carly','Guido','Steve']
                      })
      display(df1,df5,pd.merge(df1,df5)) #多对多
      display(pd.concat([df1,df5]))
      
      输出：
      	employee	group
      0	Bob		Accounting
      1	Jake		Engineering
      2	Lisa		Engineering
      
      	group		supervisor
      0	Engineering	Carly
      1	Engineering	Guido
      2		HR			Steve
      
      employee	group	supervisor
      0	Jake	Engineering	Carly
      1	Jake	Engineering	Guido
      2	Lisa	Engineering	Carly
      3	Lisa	Engineering	Guido
      
      
         	employee			group		supervisor
      0		Bob			Accounting		NaN
      1		Jake			Engineering		NaN
      2		Lisa			Engineering		NaN
      0		NaN			Engineering		Carly
      1		NaN			Engineering		Guido
      2		NaN					HR			Steve
      ```

      ```
      	employee	group
      0	Bob		Accounting
      1	Jake		Engineering
      2	Lisa		Engineering
      ​		group		supervisor
      0	Engineering	Carly
      1	Engineering	Guido
      2	HR			Steve
      ```

    - key的规范化

      - 使用on=显式指定哪一列为key

        ```
        df1 = DataFrame({'employee':['Jack',"Summer","Steve"],
                         'group':['Accounting','Finance','Marketing']})
        
        df2 = DataFrame({'employee':['Jack','Bob',"Jake"],
                         'hire_date':[2003,2009,2012],
                        'group':['Accounting','sell','ceo']})
        display(df1,df2,pd.merge(df1,df2),pd.merge(df1,df2,on = 'employee'))
        
        输出：
        
        employee	group
        0	Jack	Accounting
        1	Summer	Finance
        2	Steve	Marketing
        
        
        employee	group	hire_date
        0	Jack	Accounting	2003
        1	Bob	sell	2009
        2	Jake	ceo	2012
        
        employee	group	hire_date
        0	Jack	Accounting	2003
        
        	employee	 group_x	group_y	hire_date
        0	Jack 		Accounting	 Accounting	2003
        ```

        - 两个表存在多个相同key时，可以指定某一列为合并列
        - on也可以指定多个列, 如on=('employee', 'group')

      - 使用left_on和right_on指定左右两边的列作为key	

        ```
        df1 = DataFrame({'employee':['Bobs','Linda','Bill'],
                        'group':['Accounting','Product','Marketing'],
                       'hire_date':[1998,2017,2018]})
        df5 = DataFrame({'name':['Lisa','Bobs','Bill'],
                        'hire_dates':[1998,2016,2007]})
        
        display(df1,df5,pd.merge(df1,df5,left_on = 'employee',right_on = 'name'))
        
        
        输出：
        	employee	group		hire_date
        0		Bobs	Accounting	1998
        1		Linda	Product			2017
        2		Bill	Marketing			2018
        
         hire_dates	 	name
        0	1998			Lisa
        1	2016			Bobs
        2	2007			Bill
        
        
        employee	group	hire_date	hire_dates	name
        0	Bobs	Accounting	1998	2016	Bobs
        1	Bill	Marketing	2018	2007	Bill
        ```

        > 如果左表与右表的字段名不同，但意义相同时，需要指定left_on和right_on
        > ​dataframe.drop('行或列标签', axis=0/1) 删除某一行(axis=0)或一列(axis=1)				

    - 内合并与外合并

      - 内合并：只保留两者都有的key（默认模式）

        ```
        df6 = DataFrame({'name':['Peter','Paul','Mary'],
                       'food':['fish','beans','bread']}
                       )
        df7 = DataFrame({'name':['Mary','Joseph'],
                        'drink':['wine','beer']})
        display(df6,df7,pd.merge(df6,df7))
        
        输出：
        
        	food	name
        0	fish	Peter
        1	beans	Paul
        2	bread	Mary
        
        	drink	name
        0	wine	Mary
        1	beer	Joseph
        
        	food		name	drink
        0	bread	Mary		winee
        ```

        

      - 外合并 how='outer'：补NaN

        ```python
        df6 = DataFrame({'name':['Peter','Paul','Mary'],
                       'food':['fish','beans','bread']}
                       )
        df7 = DataFrame({'name':['Mary','Joseph'],
                        'drink':['wine','beer']})
        display(df6,df7,pd.merge(df6,df7,how='outer'))
        
        
        输出：
        	food	name
        0	fish	Peter
        1	beans	Paul
        2	bread	Mary
        
        	drink	name
        0	wine	Mary
        1	beer	Joseph
        
         	 food		name		drink 
        0	fish			Peter		NaN  
        1	beans		Paul			NaN  
        2	bread		Mary	 		wine 
        3	NaN		  Joseph		beer  
        ```

        

      - 左合并、右合并：how='left'，how='right'

    - 列冲突的解决

      - 当列冲突时，即有多个列名称相同时，需要使用on=来指定哪一个列作为key

        ```python
        df8 = DataFrame({'name':['Peter','Paul','Mary'],
                        'rank':[1,2,3]})
        df9 = DataFrame({'name':['Peter','Paul','Mary'],
                        'rank':[5,6,7]})
        display(df8,df9,pd.merge(df8,df9,on = 'name',suffixes=['_L','_R']))  
        
        输出：
        	name	rank
        0	Peter	1
        1	Paul		2
        2	Mary		3
        name	rank
        0	Peter	5
        1	Paul		6
        2	Mary		7
        
        	name	rank_L	rank_R
        0	Peter		1					5			
        1	Paul			2					6			
        2	Mary			3					7			
        ```

        - suffixes 指定冲突列标签后辍名， 默认为x,y
          ​					

## pandas案例

### 		美国各州人口数据分析

- 首先导入文件，并查看数据样本

  ```
  pop = pd.read_csv('../../data/state-population.csv')
  areas = pd.read_csv('../../data/state-areas.csv')
  abbrevs = pd.read_csv('../../data/state-abbrevs.csv')
  
  display(pop.head(),areas.head(),abbrevs.head())
  
  输出：
  state/region	ages	year	population
  0	AL	under18	2012	1117489.0
  1	AL	total	2012	4817528.0
  2	AL	under18	2010	1130966.0
  3	AL	total	2010	4785570.0
  4	AL	under18	2011	1125763.0
  
  state	area (sq. mi)
  0	Alabama	52423
  1	Alaska	656425
  2	Arizona	114006
  3	Arkansas	53182
  4	California	163707
  
  
  state	abbreviation
  0	Alabama	AL
  1	Alaska	AK
  2	Arizona	AZ
  3	Arkansas	AR
  4	California	CA
  ```

- 合并pop与abbrevs两个DataFrame，分别依据state/region列和abbreviation列来合并。为了保留所有信息，使用外合并

  ```
  merged = pd.merge(pop,abbrevs,how='outer',left_on = 'state/region',right_on = 'abbreviation')
  merged2 = pd.merge(pop,abbrevs,left_on = 'state/region',right_on = 'abbreviation')
  display(merged.head(),merged2.head())
  
  输出：
  
  state/region	ages	year	population	state	abbreviation
  0	AL	under18	2012	1117489.0	Alabama	AL
  1	AL	total	2012	4817528.0	Alabama	AL
  2	AL	under18	2010	1130966.0	Alabama	AL
  3	AL	total	2010	4785570.0	Alabama	AL
  4	AL	under18	2011	1125763.0	Alabama	AL
  
  
  state/region	ages	year	population	state	abbreviation
  0	AL	under18	2012	1117489.0	Alabama	AL
  1	AL	total	2012	4817528.0	Alabama	AL
  2	AL	under18	2010	1130966.0	Alabama	AL
  3	AL	total	2010	4785570.0	Alabama	AL
  4	AL	under18	2011	1125763.0	Alabama	AL
  ```

- 去除abbreviation的那一列（axis=1）

  ```
  merged = merged.drop('abbreviation',axis = 1)
  merged.head()
  
  输出：
  
  state/region	ages	year	population	state
  0	AL	under18	2012	1117489.0	Alabama
  1	AL	total	2012	4817528.0	Alabama
  2	AL	under18	2010	1130966.0	Alabama
  3	AL	total	2010	4785570.0	Alabama
  4	AL	under18	2011	1125763.0	Alabama
  ```

- 查看存在缺失数据的列。使用.isnull().any()，只有某一列存在一个缺失数据，就会显示True。

  ```
  merged.isnull().any()
  
  输出：
  
  state/region    False
  ages            False
  year            False
  population       True
  state            True
  dtype: bool
  ```

- 查看缺失数据的例子

  ```
  # display(merged['population'].isnull()) # 查看列population 缺失数据 如果缺失返回null
  merged[merged['population'].isnull()] #根据数据是否缺失情况显示数据，如果缺失为True，那么显示
  
  输出：
  	state/region	ages	year	population	state
  2448	PR	under18	1990	NaN	NaN
  2449	PR	total	1990	NaN	NaN
  2450	PR	total	1991	NaN	NaN
  2451	PR	under18	1991	NaN	NaN
  2452	PR	total	1993	NaN	NaN
  2453	PR	under18	1993	NaN	NaN
  2454	PR	under18	1992	NaN	NaN
  2455	PR	total	1992	NaN	NaN
  2456	PR	under18	1994	NaN	NaN
  2457	PR	total	1994	NaN	NaN
  2458	PR	total	1995	NaN	NaN
  2459	PR	under18	1995	NaN	NaN
  2460	PR	under18	1996	NaN	NaN
  2461	PR	total	1996	NaN	NaN
  2462	PR	under18	1998	NaN	NaN
  2463	PR	total	1998	NaN	NaN
  2464	PR	total	1997	NaN	NaN
  2465	PR	under18	1997	NaN	NaN
  2466	PR	total	1999	NaN	NaN
  2467	PR	under18	1999	NaN	NaN
  ```

- 找到有哪些state/region使得state的值为NaN，使用unique()去重

  ```
  # merged.loc[merged['state'].isnull(),'state/region'].unique()
  # merged[merged['state'].isnull(),'state/region']# 出问题
  # merged[merged['state'].isnull()]
  s = merged.loc[merged['state'].isnull(),'state/region']
  ss = s.unique()
  display(s,ss)
  
  输出：
  2448     PR
  2449     PR
  2450     PR
  2451     PR
  2452     PR
  2453     PR
  2454     PR
  2455     PR
  2456     PR
  2457     PR
  2458     PR
  2459     PR
  2460     PR
  2461     PR
  2462     PR
  2463     PR
  2464     PR
  2465     PR
  2466     PR
  2467     PR
  2468     PR
  2469     PR
  2470     PR
  2471     PR
  2472     PR
  2473     PR
  2474     PR
  2475     PR
  2476     PR
  2477     PR
         ... 
  2514    USA
  2515    USA
  2516    USA
  2517    USA
  2518    USA
  2519    USA
  2520    USA
  2521    USA
  2522    USA
  2523    USA
  2524    USA
  2525    USA
  2526    USA
  2527    USA
  2528    USA
  2529    USA
  2530    USA
  2531    USA
  2532    USA
  2533    USA
  2534    USA
  2535    USA
  2536    USA
  2537    USA
  2538    USA
  2539    USA
  2540    USA
  2541    USA
  2542    USA
  2543    USA
  Name: state/region, Length: 96, dtype: object
  //////////////////////////////////////////////////////////////////////////////
  array(['PR', 'USA'], dtype=obje
  ```

  

- 为找到的这些state/region的state项补上正确的值，从而去除掉state这一列的所有NaN！记住这样清除缺失数据NaN的方法！

  ```
  merged.loc[merged['state/region']=='PR','state'] = 'Puerto Rico'
  merged.loc[merged['state/region']=='USA', 'state'] = 'United States'
  merged.isnull().any()
  
  输出：
  state/region    False
  ages            False
  year            False
  population       True
  state           False
  dtype: bool
  ```

- 合并各州面积数据areas，使用左合并。思考一下为什么使用左合并？

  ```
  display(merged.shape,areas.shape)
  df1 = pd.merge(merged,areas,on = 'state')
  df2 = pd.merge(merged,areas,on = 'state',how = 'left')
  display(df1.shape,df2.shape)
  
  输出：
  (2544, 5)
  (52, 2)
  (2496, 6)
  (2544, 6)
  ```

  

- 继续寻找存在缺失数据的列

  ```
  final.isnull().any()
  输出：
  state/region     False
  ages             False
  year             False
  population        True
  state            False
  area (sq. mi)     True
  dtype: bool
  ```

  

- 我们会发现area(sq.mi)这一列有缺失数据，为了找出是哪一行，我们需要找出是哪个state没有数据	

  ```
  s1 = df2['state'][df2['area (sq. mi)'].isnull()]
  display(s1.shape)
  
  输出：
  (48,)
  ```

- 去除含有缺失数据的行

  ```
  display(df2.shape,df2.dropna(inplace = True),df2.shape)
  
  输出：
  (2544, 6)
  None
  (2476, 6)
  ```

- 找出2010年的全民人口数据

  ```
  display(df2.shape,type(df2))
  data2010 = df2.query("year == 2010 & ages == 'total'")
  data2010.head()
  
  输出：
  (2476, 6)
  pandas.core.frame.DataFrame
  
  
  Out[122]:
  state/region	ages	year	population	state	area (sq. mi)
  3	AL	total	2010	4785570.0	Alabama	52423.0
  91	AK	total	2010	713868.0	Alaska	656425.0
  101	AZ	total	2010	6408790.0	Arizona	114006.0
  189	AR	total	2010	2922280.0	Arkansas	53182.0
  197	CA	total	2010	37333601.0	California	163707.0
  ```

- 对查询结果进行处理，以state列作为新的行索引

  ```
  data2010.set_index('state',inplace=True)
  display(data2010.shape)
  data2010.head()
  
  
  输出：
  (52, 5)
  Out[134]:
  state/region	ages	year	population	area (sq. mi)
  state					
  Alabama	AL	total	2010	4785570.0	52423.0
  Alaska	AK	total	2010	713868.0	656425.0
  Arizona	AZ	total	2010	6408790.0	114006.0
  Arkansas	AR	total	2010	2922280.0	53182.0
  California	CA	total	2010	37333601.0	163707.0
  ```

- 计算人口密度。注意是Series/Series，其结果还是一个Series

  ```
  density = data2010['population'].div(data2010['area (sq. mi)']) #出发运算
  display(data2010.shape,type(data2010),type(df2),density.head(),type(density))
  
  输出：
  (52, 5)
  pandas.core.frame.DataFrame
  pandas.core.frame.DataFrame
  state
  Alabama        91.287603
  Alaska          1.087509
  Arizona        56.214497
  Arkansas       54.948667
  California    228.051342
  dtype: float64
  pandas.core.series.Series
  ```

- 排序，并找出2013年人口密度最高的五个州

  ```
  display(density.sort_values(ascending=False,inplace = False).head())
  
  输出：
  state
  District of Columbia    8898.897059
  Puerto Rico             1058.665149
  New Jersey              1009.253268
  Rhode Island             681.339159
  Connecticut              645.600649
  dtype: float64
  ```

  

- 排序，并找出2013年人口密度最低的五个州

  ```
  display(density.sort_values(ascending=False).tail())
  
  输出：
  state
  South Dakota    10.583512
  North Dakota     9.537565
  Montana          6.736171
  Wyoming          5.768079
  Alaska           1.087509
  dtype: float64
  ```

- 要点总结：
  统一用.loc[]索引
  善于使用.isnull().any()找到存在NaN的列
  善于使用.unique()确定该列中哪些key是我们需要的
  一般使用外合并、左合并，目的只有一个：宁愿该列是NaN也不要丢弃其他列的信息扩展

### Series/DataFrame运算与ndarray运算的区别

```
ndarray有广播，通过重复已有值来计算
	Series与DataFrame没有广播，如果对应index没有值，则记为NaN；
或者使用add的fill_value来补缺失值
		Series
			unique() 去重
		DataFrame
			query('name== "Lucy" & age > 20')
			head() 头标签（样本）
```

### 扩展

		1. mysql索引的使用和优化
			https://www.cnblogs.com/doudouxiaoye/p/5831449.html
		2. MySQL 约束类型
				http://www.cnblogs.com/jennyyin/p/7895400.html
# Day04-pandas-III

## pandas数据处理

### 删除重复元素

```
df = DataFrame({'color':['white','white','red','red','white'],
               'value':[2,1,3,3,2]})

display(df,df.duplicated(), df.drop_duplicates())


输出:

color	value
0	    white	2
1	    white	1
2	red	3
3	red	3
4	white	2
0    False
1    False
2    False
3     True
4     True
dtype: bool
color	value
0	white	2
1	white	1
2	red	3

```

- drop(labels, axis, index, columns)  删除行或列
- dropna(axis, how='any|all')  删除空值
- drop_duplicates(keep='first|last')  删除重复行   注：老版本中，要求列不能出现的重复的，如果重复，则指定非重复的列。
- duplicated(keep='first')  查询重复的行
  注意
  如果使用pd.concat(df1, df2, axis=1) 生成新的DataFrame中存在columns列标签相同时，
  duplicated()和drop_duplicates()都会出现问题

### 映射

#### replace(to_replace, value)函数：替换元素

```
df = DataFrame({'item':['ball','mug','pen'],
               'color':['white','rosso','verde'],
               'price':[5.56,4.20,1.30]})

newcolors = {'rosso':'red','verde':'green'}
display(df,df.replace(newcolors))

输出：
	color	item	price
0	white	ball	5.56
1	rosso	mug	4.20
2	verde	pen	1.30
color	item	price
0	white	ball	5.56
1	red	mug	4.20
2	green	pen	1.30



replace还经常用来替换NaN元素

df2 = DataFrame({'math':[100,139,np.nan],'English':[146,None,119]},index = ['张三','李四','Tom'])
newvalues = {np.nan:100}
display(df2,df2.replace(newvalues))

输出：
	English	math
张三	146.0	100.0
李四	NaN	139.0
Tom	119.0	NaN
English	math
张三	146.0	100.0
李四	100.0	139.0
Tom	119.0	100.0
```

- 分类
  - 单值替换
    - 普通替换
    - 按列指定单值替换
  - 多值替换
    - 列表替换
    - 单字典替换
  - method与limit属性
    - method
      - 对指定的值使用相邻的值填充
      - ffill  向后填充
      - bfill 向前填充
    - limit  设置填充的次数, 默认不限制
      	注： 如果对DataFrame做填充操作，需要先读取一行或一列再做处理
      				
- Series替换操作
  - s2 = Series(['Disen', 20, None, '西安'])
  - s2.replace(to_replace='Disen', value='狄哥'
  - s2.replace(to_replace=[None], value='女')
    - None值必须要以 [ ] 指定
    - 可以直接用np.nan
  - s2.replace(to_replace=[None, 'Disen'], value=['男', '战神'])
  - s2.replace(['刘备', 'Disen'], ['没有', '刘备的主公'])
    - 不存在的“刘备”，不做任何操作
  - s2.replace({'Disen': '狄森', 20: 30})
    	单字典替换
  - s2[:3] = 'Disen'
  - s2.replace('Disen', method='bfill', limit=1)   向前邻近填充1次

- DataFrame替换操作

  ```
  s3 = DataFrame(np.random.randint(1, 100, size=(5,5)), columns=tuple('ABCDE'))
  s3.loc[0, 'D'] = 'disen'
  s3.loc[1, 'B'] = 'disen'
  ```

  - s3.replace({'D': 'disen'}, '战神')
    - D为列名， 'disen'为列值， 替换成“战神”
    - 指定列替换格式 replace({columnname: datavalue},  replacevalue)

- df表

  - df.loc[['张三', '李四']].replace([150, 300], 0)

#### map()函数：

> 新建一列
> map中返回的数据是一个具体值，不能迭代

```
使用map()函数，由已有的列生成一个新列
适合处理某一单独的列


df3 = DataFrame({'color':['red','green','blue'],'project':['math','english','chemistry']})
price = {'red':5.56,'green':3.14,'chemistry':2.79}
df3['price'] = df3['color'].map(price)
display(df3)


输出：

color	project	price
0	red	math	5.56
1	green	english	3.14
2	blue	chemistry	NaN
```

- 功能

  - 可以映射新一列数据

    ```
    data = np.random.randint(1, 100, size=(5, 5))
    index = tuple('ABCDE')
    column = tuple('甲乙丙丁戊')
    s = DataFrame(data,index,column)
    s['甲'] = ['Disen', 'Jack', 'Judy', 'Mack', 'Rose']
    ```

    ```
    	s['性别'] = s['甲'].map({
        'Disen': '男',
        'Jack': '女',
        'Judy': '女',
        'Mack': '男',
        'Rose': '女'
    })
    ```

    - 新一列数据 “性别”
    - s['列名'].map() 返回指定列映射索引的信息

  - 可以使用lambda表达式

    - s['等级'] = s['丁'].map(lambda item: 'A' if item > 90 else 'B' if item > 75 else 'C' if item > 60 else 'D' )

      def price_tip(item):
          return item*.5

  - 可以使用方法，可以是自定义方法



### 异常值检测和过滤

- 使用describe()函数查看每一列的描述性统计量

  ```
  import numpy as np
  import pandas as pd
  from pandas import Series, DataFrame
  
  np.random.seed(0)
  df = DataFrame(np.random.randint(10, size = 10))
  
  display(df.head(10), df.describe())
  
  
  输出：
  	0
  0	5
  1	0
  2	3
  3	3
  4	7
  5	9
  6	3
  7	5
  8	2
  9	4
  0
  count	10.000000
  mean	4.100000
  std	2.558211
  min	0.000000
  25%	3.000000
  50%	3.500000
  75%	5.000000
  max	9.000000
  ```

  - 包含的信息
    - count
    - mean
    - std
    - min
    - max
    - 25%-%50-75% 分段比例
  - percentiles=[.65, 0.8]
    - 指定百分比段
  - include=[np.int]
    - 结果中包含的数据类型
    - 【推荐】使用numpy中的类型

- 使用std()函数可以求得DataFrame对象每一列的标准差

  ```
  df.std()
  
  
  输出：
  0    2.643651
  1    2.923088
  dtype: float64
  ```

  

- 根据每一列的标准差，对DataFrame元素进行过滤。借助any()函数，对每一列应用筛选条件

  ```
  display(df.std(),np.abs(df)>(3*df.std()),df[(np.abs(df)>df.std()*3).any(axis = 1)])
  DataFrame.any?
  
  
  
  输出：
  0    2.643651
  1    2.923088
  dtype: float64
  0	1
  0	False	False
  1	False	False
  2	False	True
  3	False	False
  4	False	False
  5	False	False
  6	True	False
  7	False	False
  8	False	False
  9	True	False
  0	1
  2	7	9
  6	8	8
  9	8	1
  ```

  ​			

### 排列

使用.take(indices, axis)函数排列
可以借助np.random.permutation(n)函数随机排列

```
df5 = DataFrame(np.arange(25).reshape(5,5))

new_order = np.random.permutation(5)

display(df5, new_order, df5.take(new_order))


输出：
	0	1	2	3	4
0	0	1	2	3	4
1	5	6	7	8	9
2	10	11	12	13	14
3	15	16	17	18	19
4	20	21	22	23	24

array([4, 2, 3, 1, 0])

0	1	2	3	4
4	20	21	22	23	24
2	10	11	12	13	14
3	15	16	17	18	19
1	5	6	7	8	9
0	0	1	2	3	4
```

随机抽样
当DataFrame规模足够大时，直接使用np.random.randint()函数，就配合take()函数实现随机抽样

```
df = DataFrame(np.random.randn(10000, 3))
sample = np.random.randint(0, len(df), size = 3)

df.take(sample)


输出：
0	1	2	3	4
0	0	1	2	3	4
2	10	11	12	13	14
4	20	21	22	23	24
```



### 数据分类处理【重点】

- 数据聚合是数据处理的最后一步，通常是要使每一个数组生成一个单一的数值。
- 数据分类处理
  - 分组：先把数据分为几组
  - 用函数处理：为不同组的数据应用不同的函数以转换数据
  - 合并：把不同组得到的结果合并起来
- 数据分类处理的核心： groupby()
  - groupby() 分组函数
    - 分组之后可以groups 属性查看分组情况
    - sum()
      - 分组之后看总和
      - s.groupby('性别')['丙'].sum().loc['男']  # 按性别分组，查看 丙的 男生的总和
    - mean()    分组之后看平均值
    - count()   统计
    - max()    最大
    - min()     最小
    - 注
      - 可以两列以上的分组
        - s.groupby(['等级', '性别']).mean()
        - 即： 先按“等级”分组，再按“性别”分组，再计算每列的平均值
      - df.reset_index() 重新构建索引
        - 把原索引变成一列
        - 如果drop参数为True时，删除行索引
      - df.set_index()  将列转成索引

### 高级数据聚合

- 可以使用pd.merge()函数 将聚合操作的计算结果添加到df的每一行

  

- 可以使用transform和apply实现相同功能

  - transform
  - apply
    - groupby()分组之后，通过transform或apply提供自定义函数实现更多的运算;
    - transform()或apply()可以使用 lambda表达式
    - transform()返回的对象调用 add_prefix('total_')添加列标签的前辍				

两个函数的区别？

- transform会自动列索引返回值， 不去重
  - transform指定函数接收的参数是Series对象
- apply会根据分组情况返回值， 去重
  - apply指定函数接收的参数是DataFrame类型

## 苹果公司股票分析

​	导包

```
import pandas as pd
import numpy as np

# 可视化
import matplotlib.pyplot as plt

%matplotlib inline
```

​	读取数据

```
apple = pd.read_csv('AAPL.csv')

apple.shape
```

检查数据类型

```
apple.dtypes
```

将'Date'这行数据转换为时间数据类型

```
apple.Date = pd.to_datetime(apple.Date)

apple['Date'].head()
```

```
pd.to_datetime('23-7-18') -> Timestamp()

Timestamp -> datetime类型
```

将'Date'设置为行索引

```
apple.set_index('Date', inplace=True)
apple.head()
```

绘制图形，字段Adj Close：已调整收盘价格

```
# 绘图
appl_open = apple['Adj Close'].plot(title = "Apple Stock")

# 设置图形大小
fig = appl_open.get_figure()

fig.set_size_inches(13.5,  9)
```

​	![image-20200117142231263](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117142231263.png)

扩展
	
解决matplotlib 的中文乱码问题
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif']=['SimHei']    # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus']= False   #用来正常显示负号

## pandas绘图函数

### 线型图 line

- 简单的Series图标示例

  ```
  import numpy as np
  import pandas as pd
  from pandas import Series,DataFrame
  import matplotlib.pyplot as plt
  
  np.random.seed(0)
  s = Series(np.random.randn(10).cumsum(), 
                    index = np.arange(0,100,10))
  
  s.plot()
  plt.show(s.plot())
  ```

  ![image-20200117142636119](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117142636119.png)

- 简单的DataFrame图标示例

  ```
  np.random.seed(0)
  df = DataFrame( np.random.randn(10,4).cumsum(0),
                columns= ['A','B','C','D'],
                index = np.arange(0,100,10))
  
  plt.show(df.plot())
  ```

  ​	![image-20200117142708500](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117142708500.png)	

### 柱状图 bar/barh

- 水平和垂直柱状图

  ```
  fig, axes = plt.subplots(2,1)
  data = Series(np.random.rand(16),
                index=[chr(c) for c in range(ord('a'),  ord('a')+16)])
  
  data.plot(kind = 'bar', ax = axes[0],color = 'b', alpha = 0.9)
  data.plot(kind = 'barh', ax = axes[1],color = 'b', alpha = 0.9)
  ```

  ![image-20200117143034430](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117143034430.png)

- DataFrame柱状图示例
  	![image-20200117143208640](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117143208640.png)		

  ![image-20200117143229405](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117143229405.png)				.plot(kind = 'bar')
  			

  - .plot(kind = 'bar', stacked = True)

### 直方图 hist 和密度图 kde

- 直方图是一种可以对值频率进行离散化显示的柱状图，通过Series的hist方法
- random随机数百分比的直方图
  			![image-20200117143846416](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117143846416.png)	

```
a = np.random.random(10)

b = a/a.sum()
s = Series(b)

plt.show(s.hist(bins = 100))     #bins直方图的柱数
```

- random随机数百分比的密度图

  ​		![image-20200117144113154](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117144113154.png)		

```
a = np.random.random(10)

b = a/a.sum()
s = Series(b)

plt.show(s.plot(kind = 'kde'))
```

- 带有密度估计的规格化直方图
  		![image-20200117144231428](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117144231428.png)		

```
%matplotlib inline

comp1 = np.random.normal(0,1,  size = 200)
comp2 = np.random.normal(10,2,  size = 200)

values = Series(np.concatenate([comp1,comp2]))

p1 = values.hist(bins = 100, alpha = 0.3, color = 'k', normed = True)
p2 = values.plot(kind = 'kde', style = '--',color = 'r')
```



### 散步图 scatter

- 一张简单散布图

  ![image-20200117144810741](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117144810741.png)			

```
df = DataFrame(np.random.randint(0,100,size = 100).reshape(50,2),  
 columns = ['A','B'])

df.plot('A', 'B', kind = 'scatter', title = 'x Vs y')
```

- 散布图矩阵
  		![image-20200117144944787](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117144944787.png)		

在多变量概率统计中，散布矩阵是用来估计多维正态分布协方差的统计量。

```
import numpy as np

import pandas as pd
from pandas import Series,DataFrame
%matplotlib inline

df = DataFrame(np.random.randn(200).reshape(50,4), columns = ['A','B','C','D'])

pd.plotting.scatter_matrix(df, diagonal = 'kde', color = 'k')

```

# day05-加载数据与scipy

## 	pandas加载数据

### 读取文本格式数据

- pandas提供了一些用于将表格型数据读取为DataFrame对象的函数

- 最常用

  - read_csv  从文件中加载带分隔符的数据，默认分隔符为逗号   sep=','
  - read_table   从文件中加载带分隔符的数据，默认分隔符为制表符  sep='\t'
  - read_excel    从Excel文件中读取数据

  ### 读取sqlite数据库数据

  - 导包

    ```
    import pandas as pd
    import sqlite3
    ```

  - 读取数据

    ```
    con = sqlite3.connect("../data/weather_2012.sqlite")
    df = pd.read_sql("SELECT * from weather_2012 LIMIT 3",  con)
    df
    ```

    - 创建连接
      - con = sqlite3.connect("data/weather_2012.sqlite")
      - 字典表： SQLITE_MASTER
    - 读取数据
      -  pd.read_sql(sql, conn)

  - 设置index_col

    - 指定哪一列为索引
    -  pd.read_sql(sql, conn, index_col='列名')

  - 写数据

    ```
    weather_df = pd.read_csv('../data/weather_2012.csv')
    
    con = sqlite3.connect("../data/test_db.sqlite")
    
    con.execute("DROP TABLE IF EXISTS weather_2017")
    
    weather_df.to_sql("weather_2017", con)
    ```

    - df.to_sql(table_name, conn)

    - 扩展: mysql数据库写入时，需要sqlalchemy创建连接	

      ```
      from sqlalchemy import create_engine
      conn = create_engine('mysql+pymysql://root:root@127.0.0.1/stu?charset=utf8')
      ```

      ```
      from sqlalchemy import create_engine
      conn = create_engine('mysql+pymysql://root:root@127.0.0.1/stu?charset=utf8')
      ```

### 读取网络数据

- url

https://raw.githubusercontent.com/datasets/investor-flow-of-funds-us/master/data/weekly.csv
https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data

没有标题

- pd.read_csv(, header=None, prefix='V'

- 获取数据
  - df = pd.read_csv(url)
    - sep 指定分隔符
    - header 指定标题头   如果数据没有头，可以设置为None
    - prefix 如果标题头设置为None, 则自动以prefix 指定的字符开头＋序号
      		

### 数据获取地址

​			https://www.kaggle.com/datasets
​			http://archive.ics.uci.edu/ml/index.php
​			https://archive.ics.uci.edu/ml/index.php
​			https://archive.ics.uci.edu/ml/machine-learning-databases/

## 	透视表和交叉表

> 什么是数据透视表

- 数据透视表（Pivot Table）是一种交互式的表，可以进行某些计算，如求和与计数等。
- 所进行的计算与数据跟数据透视表中的排列有关。
- 之所以称为数据透视表，是因为可以动态地改变它们的版面布置，以便按照不同方式分析数据，
  也可以重新安排行号、列标和页字段。
- 每一次改变版面布置时，数据透视表会立即按照新的布置重新计算数据。
- 另外，如果原始数据发生更改，则可以更新数据透视表。

### 透视表

- 行分组透视表	

  ```
  df = DataFrame({'size':np.random.randn(4),
                  'height':np.random.randn(4),
                  'weight':np.random.randn(4),
                  'smoke':['No','Yes','No','Yes'],
                  'sex':['male','female','female','male']})
  
  df2 = df.pivot_table(index = ['smoke'])
  ```

  ​	![image-20200117154943590](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117154943590.png)

​			df.pivot_table(index = ['smoke'])

- 列分组透视表
  			![image-20200117155007566](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117155007566.png)
  	pivot_table(columns = ['smoke'])
- 行列分组透视表
  			![image-20200117155026226](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117155026226.png)
  	df.pivot_table(index = 'sex',columns = 'smoke', aggfunc= sum, fill_value=0)

### 交叉表

```
df = DataFrame({'gender':['male','female','female','male','male','female','female','female','male','female'],
                'hand':['left','right','right','right','right','right','left','right','right','right']})

display(df,  pd.crosstab(df.gender, df.hand, margins=True))


数据：
hand			left		right		All
gender	
female		1			5				6
male			1			3				4
All				2			8				10
```

pd.crosstab()

## Scipy

- Scipy简介
  - Scipy依赖于Numpy
  - Scipy提供了真正的矩阵
  - Scipy包含的功能：最优化、线性代数、积分、插值、拟合、特殊函数、快速傅里叶变换、信号处理、图像处理、常微分方程求解器等
  - Scipy是高端科学计算工具包
  - Scipy由一些特定功能的子模块组成
    	![image-20200117155355558](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117155355558.png)		
- 登月图片消噪

```
#模块用来计算快速傅里叶变换

import scipy.fftpack as fftpack
import matplotlib.pyplot as plt
%matplotlib inline


data = plt.imread('moonlanding.png')

data2 = fftpack.fft2(data)

data3 = np.where(np.abs(data2)>8e2, 0, data2)

data4 = fftpack.ifft2(data3)

data5 = np.real(data4)

plt.figure(figsize=(12, 9))

plt.imshow(data5,   cmap = 'gray')
```

-  图片灰度化处理
  - 概念
    灰度化处理就是将一幅色彩图像转化为灰度图像的过程。彩色图像分为R，G，B三个分量，分别显示出红绿蓝等各种颜色，灰度化就是使彩色的R，G，B分量相等的过程。灰度值大的像素点比较亮（像素值最大为255，为白色），反之比较暗（像素最下为0，为黑色）。
    - 三种方式
      - 最大值法
        ![image-20200117155837825](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117155837825.png)						
        data.max(axis = 2)
      - 平均值法
        			![image-20200117155911679](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117155911679.png)
        						data.mean(axis = 2)
      - 加权平均法
        		![image-20200117160003706](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117160003706.png)				
        	np.dot(data,[0.299,0.587,0.114])
  - plt.imshow(datas,   cmap='gray')
    - cmap按指定颜色做映射    注意：datas是二维ndarray， 可以使用cmap
    - 可用颜色较多： gray, spring, summer等
  - face = scipy.misc.face()
    - 从 scipy.misc中读取face图片
    - face图片是三维数组  如 (1024, 720, 3)
  - plt.imshow(face)
    	显示图片
  - plt.imshow(face.max(axis=2),cmap='gray')
    	取最大值
  - plt.imshow(face.mean(axis=2),cmap='gray')   # 取平均值
  - n = np.array([0.3,0.4, 0.3])
  - plt.imshow(np.dot(face, n),cmap='gray')  # 加权平均 [0,3,0.4,0.3]
- scipy.ffpack模块
  - scipy.fftpack模块用来计算快速傅里叶变换
    					时间域 -> 频域
  - 速度比传统傅里叶变换更快，是对之前算法的改进
  - 图片是二维数据，注意使用fftpack的二维转换方法
    - fft2, ifft2
    - 步骤
      - 将时域转成频域的数	fft_moon = fft2(moon)
      - 消噪（去掉高频次的数据）np.where( np.abs(fft_moon) > 8e2, 0, fft_moon) 
      - 将频域的数据转成时域，并提取实数部分  ifft_moon = np.real(ifft2(fft_moon))

- 高数积分

  ```
  2*x1  - x2^2 = 1
  x1^2 - x2 = 2
  ```

  ![](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117161234463.png)

  ​																			什么是积分				

  

  ![image-20200117161320539](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117161320539.png)

  

  

  ![image-20200117161349045](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117161349045.png)

  

  ​		![image-20200117161740397](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117161740397.png)

  ![image-20200117161802137](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200117161802137.png)

​	

- 绘制圆

  ```
  f = lambda x : (1 - x**2)**0.5
  
  import numpy as np
  import matplotlib.pyplot as plt
  
  x = np.linspace(-1,1,1000)
  
  plt.figure(figsize = (4, 4))
  plt.plot(x, f(x), '-', x, -f(x)  , '-',  color = 'r')
  
  plt.show()
  ```

  ​		

  - plt.plot()格式说明
    - plot(x)
    - plot(x, y)
    - plot(x, y, 'g^', x2,y2, 'b:')

```
线型 style 
'-'            solid line style

'--'            dashed line style
'-.'            dash-dot line style
':'             dotted line style
形状 marker

'.'             point marker
','             pixel marker
'o'             circle marker
'v'             triangle_down marker
'^'             triangle_up marker
'<'             triangle_left marker
'>'             triangle_right marker
'1'             tri_down marker
'2'             tri_up marker
'3'             tri_left marker
'4'             tri_right marker
's'             square marker
'p'             pentagon marker
'*'             star marker
'h'             hexagon1 marker
'H'             hexagon2 marker
'+'             plus marker
'x'             x marker
'D'             diamond marker
'd'             thin_diamond marker
'|'             vline marker
'_'             hline marker

颜色字符
							'b'         blue
'g'         green
'r'         red
'c'         cyan    蓝绿色
'm'        magenta 品红
'y'         yellow
'k'         black
'w'         white

```

使用Scipy.integrate.quad()来进行计算半圆的面积

```
from scipy import integrate

def g(x):
    return (1- x**2)**0.5

pi_2, err = integrate.quad(g, -1, 1)

print(pi_2, err)
```

- Scipy文件输入/输出
  - 导包
    - from scipy import io
    - import numpy as np
  - scipy的文件格式
    - mat的二进制文件
    - *.mat 
  - 写入文件
    - io.savemat('a.mat', mdict={'d1': np.random.ones(1000)})
  - 读取文件
    - data = spio.loadmat('a.mat')
      data['d1']

- 图片处理ndimage
  			使用Scipy中的ndimage进行处理
  				导包
  					from scipy import misc, ndimage
  				导包提取数据处理数据
  					原始图片 灰度提取
  						face = misc.face(gray=True)
  					移动图片坐标
  						ndimage.shift(input, shift=(x, y), mode)
  						如
  							ndimage.shift(face, shift=50)
  							ndimage.shift(face, (-200, 0), mode='wrap')
  								指定 mode 模式
  								mode可用
  									constant
  										常值
  									nearest
  										附近
  									reflect
  										反射
  									mirror
  										镜面/倒影
  									wrap
  										移动出去的内容，外围
  					旋转图片
  						ndimage.rotate(input, angle, mode)
  							angle 角度是正值，则逆时针旋转
  							angle 角度是负值，则顺时针旋转
  						如
  							ndimage.rotate(face, -30)
  							ndimage.rotate(face, -30, mode='nearest')
  					缩放图片
  						ndimage.zoom(face, 0.5)
  					切割图片
  						face[10:-10, 50:-50]
  				绘制图片
  					

- 图片过滤: 使图片变清楚
  			导包处理滤波
  				ndimage中的高斯滤波、中值滤波
  					高斯滤波 gaussian
  						参数sigma：高斯核的标准偏差
  					中值滤波 median
  						参数size
  							给出在每个元素上从输入数组中取出的形状位置
  							定义过滤器功能的输入
  				signal  包下维纳滤波
  					wiener() 参数mysize：滤镜尺寸的标量
  			绘制图片